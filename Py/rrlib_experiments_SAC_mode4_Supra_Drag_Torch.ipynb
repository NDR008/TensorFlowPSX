{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Gym Environment\n",
    "\n",
    "```py\n",
    "my_config[\"interface\"] = MyGranTurismoRTGYM\n",
    "my_config[\"time_step_duration\"] = 0.05 # when to give up\n",
    "my_config[\"start_obs_capture\"] = 0.05 # when to capture \n",
    "my_config[\"time_step_timeout_factor\"] = 1.0 # how late is OK\n",
    "my_config[\"act_buf_len\"] = 3 # how many past actions\n",
    "my_config[\"reset_act_buf\"] = True # resect action buffer on reset\n",
    "my_config[\"benchmark\"] = True\n",
    "my_config[\"benchmark_polyak\"] = 0.2\n",
    "```\n",
    "\n",
    "This section needs to be setup for any method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugAsGym = False\n",
    "testResult = False\n",
    "\n",
    "from myRTClass import MyGranTurismoRTGYM, DEFAULT_CONFIG_DICT\n",
    "import gymnasium\n",
    "\n",
    "my_config = DEFAULT_CONFIG_DICT\n",
    "my_config[\"interface\"] = MyGranTurismoRTGYM\n",
    "my_config[\"time_step_duration\"] = 0.1\n",
    "my_config[\"start_obs_capture\"] = 0.1\n",
    "my_config[\"time_step_timeout_factor\"] = 1.0\n",
    "my_config[\"ep_max_length\"] = 224\n",
    "my_config[\"act_buf_len\"] = 3\n",
    "my_config[\"reset_act_buf\"] = False\n",
    "my_config[\"benchmark\"] = True\n",
    "my_config[\"benchmark_polyak\"] = 0.2\n",
    "\n",
    "my_config[\"interface_kwargs\"] = {\n",
    "  'debugFlag': False, # do not use render() while True\n",
    "  'img_hist_len': 3,\n",
    "  'modelMode': 4,\n",
    "  'agent' : 'SAC',\n",
    "  #  [42, 42, K], [84, 84, K], [10, 10, K], [240, 320, K] and  [480, 640, K]\n",
    "  'imageWidth' : 42, # there is a default Cov layer for PPO with 240 x 320\n",
    "  'imageHeight' : 42,\n",
    "  'trackChoice' : 3, # 1 is High Speed Ring, 2 is 0-400m in MR2, #3 is 0-400m in Supra, #4 is test ring\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debugAsGym:\n",
    "    env = gymnasium.make(\"real-time-gym-v1\", config=my_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debugAsGym:\n",
    "    env.reset()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register the environment in a way that RLlib is happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not debugAsGym and not testResult:\n",
    "    def env_creator(env_config):\n",
    "        env = gymnasium.make(\"real-time-gym-v1\", config=env_config)\n",
    "        return env  # return an env instance\n",
    "\n",
    "    from ray.tune.registry import register_env\n",
    "    register_env(\"gt-rtgym-env-v1\", lambda config: env_creator(my_config)) # better way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not debugAsGym and not testResult:\n",
    "    import ray\n",
    "    ray.shutdown()\n",
    "    ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not debugAsGym and not testResult:\n",
    "    from ray.rllib.algorithms.sac import SACConfig\n",
    "    from ray.rllib.algorithms.ppo import PPOConfig\n",
    "    from ray.rllib.algorithms.a2c import A2CConfig\n",
    "\n",
    "    algo = (\n",
    "        SACConfig()\n",
    "        .resources(\n",
    "            num_gpus=1,\n",
    "            num_gpus_per_learner_worker=1,\n",
    "            #num_cpus_for_local_worker=1,\n",
    "            num_cpus_per_learner_worker=16,\n",
    "            num_learner_workers=1\n",
    "            )\n",
    "        .rollouts(\n",
    "            #num_rollout_workers=1,\n",
    "            enable_connectors=True,\n",
    "            batch_mode=\"truncate_episodes\",\n",
    "            #num_envs_per_worker=1\n",
    "            #rollout_fragment_length=256\n",
    "            )\n",
    "        .framework(\n",
    "            framework=\"torch\",\n",
    "            #eager_tracing=True,\n",
    "            )\n",
    "        .environment(\n",
    "            env=\"gt-rtgym-env-v1\",\n",
    "            disable_env_checking=True,\n",
    "            render_env=False,\n",
    "            )\n",
    "        .training(\n",
    "            #lr=tune.grid_search([0.01, 0.001, 0.0001])\n",
    "            #lambda_=0.95,\n",
    "            #gamma=0.99,\n",
    "            #sgd_minibatch_size=128,\n",
    "            train_batch_size=155,\n",
    "            #num_sgd_iter=8,\n",
    "            #clip_param=0.2,\n",
    "            #model={\"fcnet_hiddens\": [1, 8]},\n",
    "        )\n",
    "        .build()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nadir\\anaconda3\\envs\\GTAI2\\lib\\site-packages\\rtgym\\envs\\real_time_env.py:376: UserWarning: Time-step timed out. Elapsed since last time-step: 0.3062803000000258\n",
      "  warnings.warn(f\"Time-step timed out. Elapsed since last time-step: {now - self.__t_end}\")\n",
      "c:\\Users\\nadir\\anaconda3\\envs\\GTAI2\\lib\\site-packages\\rtgym\\envs\\real_time_env.py:376: UserWarning: Time-step timed out. Elapsed since last time-step: 0.32452249999994365\n",
      "  warnings.warn(f\"Time-step timed out. Elapsed since last time-step: {now - self.__t_end}\")\n",
      "c:\\Users\\nadir\\anaconda3\\envs\\GTAI2\\lib\\site-packages\\rtgym\\envs\\real_time_env.py:376: UserWarning: Time-step timed out. Elapsed since last time-step: 0.3136319000000185\n",
      "  warnings.warn(f\"Time-step timed out. Elapsed since last time-step: {now - self.__t_end}\")\n",
      "c:\\Users\\nadir\\anaconda3\\envs\\GTAI2\\lib\\site-packages\\rtgym\\envs\\real_time_env.py:376: UserWarning: Time-step timed out. Elapsed since last time-step: 0.32932049999988067\n",
      "  warnings.warn(f\"Time-step timed out. Elapsed since last time-step: {now - self.__t_end}\")\n",
      "c:\\Users\\nadir\\anaconda3\\envs\\GTAI2\\lib\\site-packages\\rtgym\\envs\\real_time_env.py:376: UserWarning: Time-step timed out. Elapsed since last time-step: 0.3349653999999873\n",
      "  warnings.warn(f\"Time-step timed out. Elapsed since last time-step: {now - self.__t_end}\")\n",
      "c:\\Users\\nadir\\anaconda3\\envs\\GTAI2\\lib\\site-packages\\rtgym\\envs\\real_time_env.py:376: UserWarning: Time-step timed out. Elapsed since last time-step: 0.31001470000001063\n",
      "  warnings.warn(f\"Time-step timed out. Elapsed since last time-step: {now - self.__t_end}\")\n",
      "c:\\Users\\nadir\\anaconda3\\envs\\GTAI2\\lib\\site-packages\\rtgym\\envs\\real_time_env.py:376: UserWarning: Time-step timed out. Elapsed since last time-step: 0.30924879999997756\n",
      "  warnings.warn(f\"Time-step timed out. Elapsed since last time-step: {now - self.__t_end}\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mj:\\git\\TensorFlowPSX\\Py\\rrlib_experiments_SAC_mode4_Supra_Drag_Torch.ipynb Cell 9\u001b[0m in \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/j%3A/git/TensorFlowPSX/Py/rrlib_experiments_SAC_mode4_Supra_Drag_Torch.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m N \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/j%3A/git/TensorFlowPSX/Py/rrlib_experiments_SAC_mode4_Supra_Drag_Torch.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(N):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/j%3A/git/TensorFlowPSX/Py/rrlib_experiments_SAC_mode4_Supra_Drag_Torch.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     result \u001b[39m=\u001b[39m algo\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m      <a href='vscode-notebook-cell:/j%3A/git/TensorFlowPSX/Py/rrlib_experiments_SAC_mode4_Supra_Drag_Torch.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLoop: \u001b[39m\u001b[39m\"\u001b[39m, n)\n\u001b[0;32m      <a href='vscode-notebook-cell:/j%3A/git/TensorFlowPSX/Py/rrlib_experiments_SAC_mode4_Supra_Drag_Torch.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mif\u001b[39;00m n \u001b[39m%\u001b[39m \u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\nadir\\anaconda3\\envs\\GTAI2\\lib\\site-packages\\ray\\tune\\trainable\\trainable.py:381\u001b[0m, in \u001b[0;36mTrainable.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    379\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m    380\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 381\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m    382\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    383\u001b[0m     skipped \u001b[39m=\u001b[39m skip_exceptions(e)\n",
      "File \u001b[1;32mc:\\Users\\nadir\\anaconda3\\envs\\GTAI2\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py:792\u001b[0m, in \u001b[0;36mAlgorithm.step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    784\u001b[0m     (\n\u001b[0;32m    785\u001b[0m         results,\n\u001b[0;32m    786\u001b[0m         train_iter_ctx,\n\u001b[0;32m    787\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_one_training_iteration_and_evaluation_in_parallel()\n\u001b[0;32m    788\u001b[0m \u001b[39m# - No evaluation necessary, just run the next training iteration.\u001b[39;00m\n\u001b[0;32m    789\u001b[0m \u001b[39m# - We have to evaluate in this training iteration, but no parallelism ->\u001b[39;00m\n\u001b[0;32m    790\u001b[0m \u001b[39m#   evaluate after the training iteration is entirely done.\u001b[39;00m\n\u001b[0;32m    791\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 792\u001b[0m     results, train_iter_ctx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_one_training_iteration()\n\u001b[0;32m    794\u001b[0m \u001b[39m# Sequential: Train (already done above), then evaluate.\u001b[39;00m\n\u001b[0;32m    795\u001b[0m \u001b[39mif\u001b[39;00m evaluate_this_iter \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mevaluation_parallel_to_training:\n",
      "File \u001b[1;32mc:\\Users\\nadir\\anaconda3\\envs\\GTAI2\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py:2811\u001b[0m, in \u001b[0;36mAlgorithm._run_one_training_iteration\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2809\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timers[TRAINING_ITERATION_TIMER]:\n\u001b[0;32m   2810\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39m_disable_execution_plan_api:\n\u001b[1;32m-> 2811\u001b[0m         results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step()\n\u001b[0;32m   2812\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2813\u001b[0m         results \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_exec_impl)\n",
      "File \u001b[1;32mc:\\Users\\nadir\\anaconda3\\envs\\GTAI2\\lib\\site-packages\\ray\\rllib\\algorithms\\dqn\\dqn.py:452\u001b[0m, in \u001b[0;36mDQN.training_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    450\u001b[0m     train_results \u001b[39m=\u001b[39m train_one_step(\u001b[39mself\u001b[39m, train_batch)\n\u001b[0;32m    451\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 452\u001b[0m     train_results \u001b[39m=\u001b[39m multi_gpu_train_one_step(\u001b[39mself\u001b[39;49m, train_batch)\n\u001b[0;32m    454\u001b[0m \u001b[39m# Update replay buffer priorities.\u001b[39;00m\n\u001b[0;32m    455\u001b[0m update_priorities_in_replay_buffer(\n\u001b[0;32m    456\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlocal_replay_buffer,\n\u001b[0;32m    457\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig,\n\u001b[0;32m    458\u001b[0m     train_batch,\n\u001b[0;32m    459\u001b[0m     train_results,\n\u001b[0;32m    460\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\nadir\\anaconda3\\envs\\GTAI2\\lib\\site-packages\\ray\\rllib\\execution\\train_ops.py:163\u001b[0m, in \u001b[0;36mmulti_gpu_train_one_step\u001b[1;34m(algorithm, train_batch)\u001b[0m\n\u001b[0;32m    158\u001b[0m         permutation \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mpermutation(num_batches)\n\u001b[0;32m    159\u001b[0m         \u001b[39mfor\u001b[39;00m batch_index \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_batches):\n\u001b[0;32m    160\u001b[0m             \u001b[39m# Learn on the pre-loaded data in the buffer.\u001b[39;00m\n\u001b[0;32m    161\u001b[0m             \u001b[39m# Note: For minibatch SGD, the data is an offset into\u001b[39;00m\n\u001b[0;32m    162\u001b[0m             \u001b[39m# the pre-loaded entire train batch.\u001b[39;00m\n\u001b[1;32m--> 163\u001b[0m             results \u001b[39m=\u001b[39m policy\u001b[39m.\u001b[39;49mlearn_on_loaded_batch(\n\u001b[0;32m    164\u001b[0m                 permutation[batch_index] \u001b[39m*\u001b[39;49m per_device_batch_size, buffer_index\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m\n\u001b[0;32m    165\u001b[0m             )\n\u001b[0;32m    167\u001b[0m             learner_info_builder\u001b[39m.\u001b[39madd_learn_on_batch_results(results, policy_id)\n\u001b[0;32m    169\u001b[0m \u001b[39m# Tower reduce and finalize results.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nadir\\anaconda3\\envs\\GTAI2\\lib\\site-packages\\ray\\rllib\\policy\\torch_policy.py:608\u001b[0m, in \u001b[0;36mTorchPolicy.learn_on_loaded_batch\u001b[1;34m(self, offset, buffer_index)\u001b[0m\n\u001b[0;32m    605\u001b[0m     batch_fetches[\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtower_\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mcustom_metrics\u001b[39m\u001b[39m\"\u001b[39m: custom_metrics}\n\u001b[0;32m    607\u001b[0m \u001b[39m# Do the (maybe parallelized) gradient calculation step.\u001b[39;00m\n\u001b[1;32m--> 608\u001b[0m tower_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_multi_gpu_parallel_grad_calc(device_batches)\n\u001b[0;32m    610\u001b[0m \u001b[39m# Mean-reduce gradients over GPU-towers (do this on CPU: self.device).\u001b[39;00m\n\u001b[0;32m    611\u001b[0m all_grads \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\nadir\\anaconda3\\envs\\GTAI2\\lib\\site-packages\\ray\\rllib\\policy\\torch_policy.py:1174\u001b[0m, in \u001b[0;36mTorchPolicy._multi_gpu_parallel_grad_calc\u001b[1;34m(self, sample_batches)\u001b[0m\n\u001b[0;32m   1170\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevices) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig[\u001b[39m\"\u001b[39m\u001b[39m_fake_gpus\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m   1171\u001b[0m     \u001b[39mfor\u001b[39;00m shard_idx, (model, sample_batch, device) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\n\u001b[0;32m   1172\u001b[0m         \u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_gpu_towers, sample_batches, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevices)\n\u001b[0;32m   1173\u001b[0m     ):\n\u001b[1;32m-> 1174\u001b[0m         _worker(shard_idx, model, sample_batch, device)\n\u001b[0;32m   1175\u001b[0m         \u001b[39m# Raise errors right away for better debugging.\u001b[39;00m\n\u001b[0;32m   1176\u001b[0m         last_result \u001b[39m=\u001b[39m results[\u001b[39mlen\u001b[39m(results) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\nadir\\anaconda3\\envs\\GTAI2\\lib\\site-packages\\ray\\rllib\\policy\\torch_policy.py:1117\u001b[0m, in \u001b[0;36mTorchPolicy._multi_gpu_parallel_grad_calc.<locals>._worker\u001b[1;34m(shard_idx, model, sample_batch, device)\u001b[0m\n\u001b[0;32m   1114\u001b[0m \u001b[39m# Recompute gradients of loss over all variables.\u001b[39;00m\n\u001b[0;32m   1115\u001b[0m loss_out[opt_idx]\u001b[39m.\u001b[39mbackward(retain_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m   1116\u001b[0m grad_info\u001b[39m.\u001b[39mupdate(\n\u001b[1;32m-> 1117\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mextra_grad_process(opt, loss_out[opt_idx])\n\u001b[0;32m   1118\u001b[0m )\n\u001b[0;32m   1120\u001b[0m grads \u001b[39m=\u001b[39m []\n\u001b[0;32m   1121\u001b[0m \u001b[39m# Note that return values are just references;\u001b[39;00m\n\u001b[0;32m   1122\u001b[0m \u001b[39m# Calling zero_grad would modify the values.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nadir\\anaconda3\\envs\\GTAI2\\lib\\site-packages\\ray\\rllib\\policy\\policy_template.py:365\u001b[0m, in \u001b[0;36mbuild_policy_class.<locals>.policy_cls.extra_grad_process\u001b[1;34m(self, optimizer, loss)\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[39m\"\"\"Called after optimizer.zero_grad() and loss.backward() calls.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[39mAllows for gradient processing before optimizer.step() is called.\u001b[39;00m\n\u001b[0;32m    362\u001b[0m \u001b[39mE.g. for gradient clipping.\u001b[39;00m\n\u001b[0;32m    363\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    364\u001b[0m \u001b[39mif\u001b[39;00m extra_grad_process_fn:\n\u001b[1;32m--> 365\u001b[0m     \u001b[39mreturn\u001b[39;00m extra_grad_process_fn(\u001b[39mself\u001b[39;49m, optimizer, loss)\n\u001b[0;32m    366\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    367\u001b[0m     \u001b[39mreturn\u001b[39;00m parent_cls\u001b[39m.\u001b[39mextra_grad_process(\u001b[39mself\u001b[39m, optimizer, loss)\n",
      "File \u001b[1;32mc:\\Users\\nadir\\anaconda3\\envs\\GTAI2\\lib\\site-packages\\ray\\rllib\\utils\\torch_utils.py:64\u001b[0m, in \u001b[0;36mapply_grad_clipping\u001b[1;34m(policy, optimizer, loss)\u001b[0m\n\u001b[0;32m     60\u001b[0m params \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mfilter\u001b[39m(\u001b[39mlambda\u001b[39;00m p: p\u001b[39m.\u001b[39mgrad \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, param_group[\u001b[39m\"\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m\"\u001b[39m]))\n\u001b[0;32m     61\u001b[0m \u001b[39mif\u001b[39;00m params:\n\u001b[0;32m     62\u001b[0m     \u001b[39m# PyTorch clips gradients inplace and returns the norm before clipping\u001b[39;00m\n\u001b[0;32m     63\u001b[0m     \u001b[39m# We therefore need to compute grad_gnorm further down (fixes #4965)\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m     global_norm \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mclip_grad_norm_(params, clip_value)\n\u001b[0;32m     66\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(global_norm, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m     67\u001b[0m         global_norm \u001b[39m=\u001b[39m global_norm\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n",
      "File \u001b[1;32mc:\\Users\\nadir\\anaconda3\\envs\\GTAI2\\lib\\site-packages\\torch\\nn\\utils\\clip_grad.py:49\u001b[0m, in \u001b[0;36mclip_grad_norm_\u001b[1;34m(parameters, max_norm, norm_type, error_if_nonfinite)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[39mif\u001b[39;00m error_if_nonfinite \u001b[39mand\u001b[39;00m torch\u001b[39m.\u001b[39mlogical_or(total_norm\u001b[39m.\u001b[39misnan(), total_norm\u001b[39m.\u001b[39misinf()):\n\u001b[0;32m     44\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m     45\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mThe total norm of order \u001b[39m\u001b[39m{\u001b[39;00mnorm_type\u001b[39m}\u001b[39;00m\u001b[39m for gradients from \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     46\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m`parameters` is non-finite, so it cannot be clipped. To disable \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     47\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mthis error and scale the gradients by the non-finite norm anyway, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     48\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mset `error_if_nonfinite=False`\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 49\u001b[0m clip_coef \u001b[39m=\u001b[39m max_norm \u001b[39m/\u001b[39m (total_norm \u001b[39m+\u001b[39;49m \u001b[39m1e-6\u001b[39;49m)\n\u001b[0;32m     50\u001b[0m \u001b[39m# Note: multiplying by the clamped coef is redundant when the coef is clamped to 1, but doing so\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[39m# avoids a `if clip_coef < 1:` conditional which can require a CPU <=> device synchronization\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[39m# when the gradients do not reside in CPU memory.\u001b[39;00m\n\u001b[0;32m     53\u001b[0m clip_coef_clamped \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mclamp(clip_coef, \u001b[39mmax\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if not debugAsGym and not testResult:\n",
    "    N = 1000\n",
    "\n",
    "    for n in range(N):\n",
    "        result = algo.train()\n",
    "        print(\"Loop: \", n)\n",
    "        if n % 10 == 0:\n",
    "            print(\"Saved\", n)\n",
    "            algo.save()\n",
    "            \n",
    "    algo.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not debugAsGym and not testResult:\n",
    "    N = 1000\n",
    "\n",
    "    for n in range(N):\n",
    "        result = algo.train()\n",
    "        print(\"Loop: \", n)\n",
    "        if n % 50 == 0:\n",
    "            print(\"Saved\", n)\n",
    "            algo.save()\n",
    "            \n",
    "    algo.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from myRTClass import MyGranTurismoRTGYM, DEFAULT_CONFIG_DICT\n",
    "import gymnasium\n",
    "\n",
    "my_config = DEFAULT_CONFIG_DICT\n",
    "my_config[\"interface\"] = MyGranTurismoRTGYM\n",
    "my_config[\"time_step_duration\"] = 0.1\n",
    "my_config[\"start_obs_capture\"] = 0.1\n",
    "my_config[\"time_step_timeout_factor\"] = 1.0\n",
    "#my_config[\"ep_max_length\"] = 224\n",
    "my_config[\"act_buf_len\"] = 3\n",
    "my_config[\"reset_act_buf\"] = False\n",
    "my_config[\"benchmark\"] = True\n",
    "my_config[\"benchmark_polyak\"] = 0.2\n",
    "\n",
    "my_config[\"interface_kwargs\"] = {\n",
    "  'debugFlag': False, # do not use render() while True\n",
    "  'img_hist_len': 3,\n",
    "  'modelMode': 4,\n",
    "  'agent' : 'PPO',\n",
    "  #  [42, 42, K], [84, 84, K], [10, 10, K], [240, 320, K] and  [480, 640, K]\n",
    "  'imageWidth' : 42, # there is a default Cov layer for PPO with 240 x 320\n",
    "  'imageHeight' : 42,\n",
    "  'trackChoice' : 3, # 1 is High Speed Ring, 2 is 0-400m, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not debugAsGym and testResult:\n",
    "    def env_creator(env_config):\n",
    "        env = gymnasium.make(\"real-time-gym-v1\", config=env_config)\n",
    "        return env  # return an env instance\n",
    "\n",
    "    from ray.tune.registry import register_env\n",
    "    register_env(\"gt-rtgym-env-v1\", lambda config: env_creator(my_config)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not debugAsGym and testResult:\n",
    "    from ray.rllib.algorithms.algorithm import Algorithm\n",
    "    algo = Algorithm.from_checkpoint(\"C:/Users/nadir/ray_results/PPO_gt-rtgym-env-v1_2023-05-19_07-37-37z3d6v2w2/checkpoint_000061\")\n",
    "    #algo = Algorithm.from_checkpoint(\"C:/Users/nadir/ray_results/PPO_gt-rtgym-env-v1_2023-05-19_07-37-37z3d6v2w2/checkpoint_002000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not debugAsGym and testResult:\n",
    "    result = algo.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not debugAsGym and testResult:\n",
    "\n",
    "    policy = algo.get_policy()\n",
    "    #print(policy.model)\n",
    "    model = policy.model\n",
    "    print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GTAI2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
