{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial AI Agent PPO w/o\n",
    "NN (working) MR2 Drag Disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    print(\"GPU is not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from myRTClass import MyGranTurismoRTGYM, DEFAULT_CONFIG_DICT\n",
    "import numpy as np\n",
    "import gymnasium\n",
    "from time import sleep\n",
    "\n",
    "my_config = DEFAULT_CONFIG_DICT\n",
    "my_config[\"interface\"] = MyGranTurismoRTGYM\n",
    "my_config[\"time_step_duration\"] = 0.05\n",
    "my_config[\"start_obs_capture\"] = 0.05\n",
    "my_config[\"time_step_timeout_factor\"] = 1.0\n",
    "my_config[\"ep_max_length\"] = 400\n",
    "my_config[\"act_buf_len\"] = 4\n",
    "my_config[\"reset_act_buf\"] = True\n",
    "my_config[\"benchmark\"] = False\n",
    "my_config[\"benchmark_polyak\"] = 0.2\n",
    "\n",
    "my_config[\"interface_kwargs\"] = {\n",
    "  'debugFlag': False, # do not use render() while True\n",
    "  'discreteAccel' : True,\n",
    "  'accelAndBrake' : False,\n",
    "  'discSteer' : True,\n",
    "  'contAccelOnly' : False,\n",
    "  'discAccelOnly' : False,\n",
    "  'modelMode': 3,\n",
    "  #  [42, 42, K], [84, 84, K], [10, 10, K], [240, 320, K] and  [480, 640, K]\n",
    "  'imageWidth' : 84, # there is a default Cov layer for PPO with 240 x 320\n",
    "  'imageHeight' : 84,\n",
    "  'trackChoice' : 0, # 0 is HS, 1 is 400m\n",
    "  'carChoice' : 0, # 0 is MR2, 1 is Supra, 2 is Civic\n",
    "  'rewardMode' : \"simplex\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_creator(env_config):\n",
    "    env = gymnasium.make(\"real-time-gym-v1\", config=env_config)\n",
    "    return env  # return an env instance\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "register_env(\"gt-rtgym-env-v1\", lambda config: env_creator(my_config)) # better way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "ray.shutdown()\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .resources(\n",
    "        num_gpus=1\n",
    "        )\n",
    "    .rollouts(\n",
    "        num_rollout_workers=1,\n",
    "        enable_connectors=True,\n",
    "        batch_mode=\"truncate_episodes\",\n",
    "        #batch_mode=\"completed_episodes\",\n",
    "        )\n",
    "    .framework(\n",
    "        framework=\"torch\",\n",
    "        )\n",
    "    .environment(\n",
    "        env=\"gt-rtgym-env-v1\",\n",
    "        disable_env_checking=True,\n",
    "        render_env=False,\n",
    "        )\n",
    "    .training(\n",
    "        train_batch_size=400,\n",
    "        model={\n",
    "            \"custom_model\": \"GTCustomNet.GTCustomNet\",\n",
    "            \"custom_model_config\": {\n",
    "                \"fcnet_hiddens\":[256,256],\n",
    "                }\n",
    "        },\n",
    "        _enable_learner_api=False\n",
    "        )\n",
    "    .rl_module( _enable_rl_module_api=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# class CustomEncoder(json.JSONEncoder):\n",
    "#     def default(self, obj):\n",
    "#         try:\n",
    "#             json.dumps(obj)\n",
    "#         except TypeError:\n",
    "#             return \"Not serializable object!\"\n",
    "\n",
    "#         return obj\n",
    "\n",
    "# config_dict = config.to_dict()\n",
    "\n",
    "# print(json.dumps(config_dict, sort_keys=True, indent=4, cls=CustomEncoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = config.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo.get_policy().model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mj:\\git\\TensorFlowPSX\\Py\\rrlibIPPO_MR2_HS_CNN.ipynb Cell 10\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/j%3A/git/TensorFlowPSX/Py/rrlibIPPO_MR2_HS_CNN.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m N \u001b[39m=\u001b[39m \u001b[39m50000\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/j%3A/git/TensorFlowPSX/Py/rrlibIPPO_MR2_HS_CNN.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(N):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/j%3A/git/TensorFlowPSX/Py/rrlibIPPO_MR2_HS_CNN.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     result \u001b[39m=\u001b[39m algo\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m      <a href='vscode-notebook-cell:/j%3A/git/TensorFlowPSX/Py/rrlibIPPO_MR2_HS_CNN.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLoop: \u001b[39m\u001b[39m\"\u001b[39m, n)\n\u001b[0;32m      <a href='vscode-notebook-cell:/j%3A/git/TensorFlowPSX/Py/rrlibIPPO_MR2_HS_CNN.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mif\u001b[39;00m n \u001b[39m%\u001b[39m \u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\nadir\\anaconda3\\envs\\GTRay2.3.0\\lib\\site-packages\\ray\\tune\\trainable\\trainable.py:381\u001b[0m, in \u001b[0;36mTrainable.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    379\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m    380\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 381\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m    382\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    383\u001b[0m     skipped \u001b[39m=\u001b[39m skip_exceptions(e)\n",
      "File \u001b[1;32mc:\\Users\\nadir\\anaconda3\\envs\\GTRay2.3.0\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py:792\u001b[0m, in \u001b[0;36mAlgorithm.step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    784\u001b[0m     (\n\u001b[0;32m    785\u001b[0m         results,\n\u001b[0;32m    786\u001b[0m         train_iter_ctx,\n\u001b[0;32m    787\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_one_training_iteration_and_evaluation_in_parallel()\n\u001b[0;32m    788\u001b[0m \u001b[39m# - No evaluation necessary, just run the next training iteration.\u001b[39;00m\n\u001b[0;32m    789\u001b[0m \u001b[39m# - We have to evaluate in this training iteration, but no parallelism ->\u001b[39;00m\n\u001b[0;32m    790\u001b[0m \u001b[39m#   evaluate after the training iteration is entirely done.\u001b[39;00m\n\u001b[0;32m    791\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 792\u001b[0m     results, train_iter_ctx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_one_training_iteration()\n\u001b[0;32m    794\u001b[0m \u001b[39m# Sequential: Train (already done above), then evaluate.\u001b[39;00m\n\u001b[0;32m    795\u001b[0m \u001b[39mif\u001b[39;00m evaluate_this_iter \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mevaluation_parallel_to_training:\n",
      "File \u001b[1;32mc:\\Users\\nadir\\anaconda3\\envs\\GTRay2.3.0\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py:2811\u001b[0m, in \u001b[0;36mAlgorithm._run_one_training_iteration\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2809\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timers[TRAINING_ITERATION_TIMER]:\n\u001b[0;32m   2810\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39m_disable_execution_plan_api:\n\u001b[1;32m-> 2811\u001b[0m         results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step()\n\u001b[0;32m   2812\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2813\u001b[0m         results \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_exec_impl)\n",
      "File \u001b[1;32mc:\\Users\\nadir\\anaconda3\\envs\\GTRay2.3.0\\lib\\site-packages\\ray\\rllib\\algorithms\\ppo\\ppo.py:432\u001b[0m, in \u001b[0;36mPPO.training_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    430\u001b[0m     train_results \u001b[39m=\u001b[39m train_one_step(\u001b[39mself\u001b[39m, train_batch)\n\u001b[0;32m    431\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 432\u001b[0m     train_results \u001b[39m=\u001b[39m multi_gpu_train_one_step(\u001b[39mself\u001b[39;49m, train_batch)\n\u001b[0;32m    434\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39m_enable_learner_api:\n\u001b[0;32m    435\u001b[0m     \u001b[39m# the train results's loss keys are pids to their loss values. But we also\u001b[39;00m\n\u001b[0;32m    436\u001b[0m     \u001b[39m# return a total_loss key at the same level as the pid keys. So we need to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[39m# passing medium to infer whcih policies to update. We could use\u001b[39;00m\n\u001b[0;32m    440\u001b[0m     \u001b[39m# policies_to_train variable that is given by the user to infer this.\u001b[39;00m\n\u001b[0;32m    441\u001b[0m     policies_to_update \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(train_results\u001b[39m.\u001b[39mkeys()) \u001b[39m-\u001b[39m {ALL_MODULES}\n",
      "File \u001b[1;32mc:\\Users\\nadir\\anaconda3\\envs\\GTRay2.3.0\\lib\\site-packages\\ray\\rllib\\execution\\train_ops.py:163\u001b[0m, in \u001b[0;36mmulti_gpu_train_one_step\u001b[1;34m(algorithm, train_batch)\u001b[0m\n\u001b[0;32m    158\u001b[0m         permutation \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mpermutation(num_batches)\n\u001b[0;32m    159\u001b[0m         \u001b[39mfor\u001b[39;00m batch_index \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_batches):\n\u001b[0;32m    160\u001b[0m             \u001b[39m# Learn on the pre-loaded data in the buffer.\u001b[39;00m\n\u001b[0;32m    161\u001b[0m             \u001b[39m# Note: For minibatch SGD, the data is an offset into\u001b[39;00m\n\u001b[0;32m    162\u001b[0m             \u001b[39m# the pre-loaded entire train batch.\u001b[39;00m\n\u001b[1;32m--> 163\u001b[0m             results \u001b[39m=\u001b[39m policy\u001b[39m.\u001b[39;49mlearn_on_loaded_batch(\n\u001b[0;32m    164\u001b[0m                 permutation[batch_index] \u001b[39m*\u001b[39;49m per_device_batch_size, buffer_index\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m\n\u001b[0;32m    165\u001b[0m             )\n\u001b[0;32m    167\u001b[0m             learner_info_builder\u001b[39m.\u001b[39madd_learn_on_batch_results(results, policy_id)\n\u001b[0;32m    169\u001b[0m \u001b[39m# Tower reduce and finalize results.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nadir\\anaconda3\\envs\\GTRay2.3.0\\lib\\site-packages\\ray\\rllib\\policy\\torch_policy_v2.py:832\u001b[0m, in \u001b[0;36mTorchPolicyV2.learn_on_loaded_batch\u001b[1;34m(self, offset, buffer_index)\u001b[0m\n\u001b[0;32m    827\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_grad_updates \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    829\u001b[0m \u001b[39mfor\u001b[39;00m i, (model, batch) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_gpu_towers, device_batches)):\n\u001b[0;32m    830\u001b[0m     batch_fetches[\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtower_\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mupdate(\n\u001b[0;32m    831\u001b[0m         {\n\u001b[1;32m--> 832\u001b[0m             LEARNER_STATS_KEY: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstats_fn(batch),\n\u001b[0;32m    833\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m: {}\n\u001b[0;32m    834\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig[\u001b[39m\"\u001b[39m\u001b[39m_enable_rl_module_api\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    835\u001b[0m             \u001b[39melse\u001b[39;00m model\u001b[39m.\u001b[39mmetrics(),\n\u001b[0;32m    836\u001b[0m             NUM_GRAD_UPDATES_LIFETIME: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_grad_updates,\n\u001b[0;32m    837\u001b[0m             \u001b[39m# -1, b/c we have to measure this diff before we do the update\u001b[39;00m\n\u001b[0;32m    838\u001b[0m             \u001b[39m# above.\u001b[39;00m\n\u001b[0;32m    839\u001b[0m             DIFF_NUM_GRAD_UPDATES_VS_SAMPLER_POLICY: (\n\u001b[0;32m    840\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_grad_updates \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m (batch\u001b[39m.\u001b[39mnum_grad_updates \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m)\n\u001b[0;32m    841\u001b[0m             ),\n\u001b[0;32m    842\u001b[0m         }\n\u001b[0;32m    843\u001b[0m     )\n\u001b[0;32m    844\u001b[0m batch_fetches\u001b[39m.\u001b[39mupdate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mextra_compute_grad_fetches())\n\u001b[0;32m    846\u001b[0m \u001b[39mreturn\u001b[39;00m batch_fetches\n",
      "File \u001b[1;32mc:\\Users\\nadir\\anaconda3\\envs\\GTRay2.3.0\\lib\\site-packages\\ray\\rllib\\algorithms\\ppo\\ppo_torch_policy.py:189\u001b[0m, in \u001b[0;36mPPOTorchPolicy.stats_fn\u001b[1;34m(self, train_batch)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[39m@override\u001b[39m(TorchPolicyV2)\n\u001b[0;32m    184\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstats_fn\u001b[39m(\u001b[39mself\u001b[39m, train_batch: SampleBatch) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, TensorType]:\n\u001b[0;32m    185\u001b[0m     \u001b[39mreturn\u001b[39;00m convert_to_numpy(\n\u001b[0;32m    186\u001b[0m         {\n\u001b[0;32m    187\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mcur_kl_coeff\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkl_coeff,\n\u001b[0;32m    188\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mcur_lr\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcur_lr,\n\u001b[1;32m--> 189\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mtotal_loss\u001b[39m\u001b[39m\"\u001b[39m: torch\u001b[39m.\u001b[39;49mmean(\n\u001b[0;32m    190\u001b[0m                 torch\u001b[39m.\u001b[39;49mstack(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_tower_stats(\u001b[39m\"\u001b[39;49m\u001b[39mtotal_loss\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[0;32m    191\u001b[0m             ),\n\u001b[0;32m    192\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mpolicy_loss\u001b[39m\u001b[39m\"\u001b[39m: torch\u001b[39m.\u001b[39mmean(\n\u001b[0;32m    193\u001b[0m                 torch\u001b[39m.\u001b[39mstack(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_tower_stats(\u001b[39m\"\u001b[39m\u001b[39mmean_policy_loss\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m    194\u001b[0m             ),\n\u001b[0;32m    195\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mvf_loss\u001b[39m\u001b[39m\"\u001b[39m: torch\u001b[39m.\u001b[39mmean(\n\u001b[0;32m    196\u001b[0m                 torch\u001b[39m.\u001b[39mstack(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_tower_stats(\u001b[39m\"\u001b[39m\u001b[39mmean_vf_loss\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m    197\u001b[0m             ),\n\u001b[0;32m    198\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mvf_explained_var\u001b[39m\u001b[39m\"\u001b[39m: torch\u001b[39m.\u001b[39mmean(\n\u001b[0;32m    199\u001b[0m                 torch\u001b[39m.\u001b[39mstack(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_tower_stats(\u001b[39m\"\u001b[39m\u001b[39mvf_explained_var\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m    200\u001b[0m             ),\n\u001b[0;32m    201\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mkl\u001b[39m\u001b[39m\"\u001b[39m: torch\u001b[39m.\u001b[39mmean(torch\u001b[39m.\u001b[39mstack(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_tower_stats(\u001b[39m\"\u001b[39m\u001b[39mmean_kl_loss\u001b[39m\u001b[39m\"\u001b[39m))),\n\u001b[0;32m    202\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mentropy\u001b[39m\u001b[39m\"\u001b[39m: torch\u001b[39m.\u001b[39mmean(\n\u001b[0;32m    203\u001b[0m                 torch\u001b[39m.\u001b[39mstack(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_tower_stats(\u001b[39m\"\u001b[39m\u001b[39mmean_entropy\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m    204\u001b[0m             ),\n\u001b[0;32m    205\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mentropy_coeff\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mentropy_coeff,\n\u001b[0;32m    206\u001b[0m         }\n\u001b[0;32m    207\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N = 50000\n",
    "\n",
    "for n in range(N):\n",
    "    result = algo.train()\n",
    "    print(\"Loop: \", n)\n",
    "    if n % 10 == 0:\n",
    "        print(\"Saved\", n)\n",
    "        algo.save()\n",
    "        \n",
    "algo.save()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "algo = Algorithm.from_checkpoint(\"C:/Users/nadir/ray_results/PPO_gt-rtgym-env-v1_2023-10-14_22-11-44ayp9l_kd/checkpoint_000881\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = algo.train() #single try"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GTAI2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
