{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Gym Environment\n",
    "\n",
    "```py\n",
    "my_config[\"interface\"] = MyGranTurismoRTGYM\n",
    "my_config[\"time_step_duration\"] = 0.05 # when to give up\n",
    "my_config[\"start_obs_capture\"] = 0.05 # when to capture \n",
    "my_config[\"time_step_timeout_factor\"] = 1.0 # how late is OK\n",
    "my_config[\"act_buf_len\"] = 3 # how many past actions\n",
    "my_config[\"reset_act_buf\"] = True # resect action buffer on reset\n",
    "my_config[\"benchmark\"] = True\n",
    "my_config[\"benchmark_polyak\"] = 0.2\n",
    "```\n",
    "\n",
    "This section needs to be setup for any method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugAsGym = False\n",
    "testResult = False\n",
    "\n",
    "from myRTClass import MyGranTurismoRTGYM, DEFAULT_CONFIG_DICT\n",
    "import gymnasium\n",
    "\n",
    "my_config = DEFAULT_CONFIG_DICT\n",
    "my_config[\"interface\"] = MyGranTurismoRTGYM\n",
    "my_config[\"time_step_duration\"] = 0.1\n",
    "my_config[\"start_obs_capture\"] = 0.1\n",
    "my_config[\"time_step_timeout_factor\"] = 1.0\n",
    "my_config[\"ep_max_length\"] = 224\n",
    "my_config[\"act_buf_len\"] = 3\n",
    "my_config[\"reset_act_buf\"] = False\n",
    "my_config[\"benchmark\"] = True\n",
    "my_config[\"benchmark_polyak\"] = 0.2\n",
    "\n",
    "my_config[\"interface_kwargs\"] = {\n",
    "  'debugFlag': False, # do not use render() while True\n",
    "  'img_hist_len': 3,\n",
    "  'modelMode': 4,\n",
    "  'agent' : 'SAC',\n",
    "  #  [42, 42, K], [84, 84, K], [10, 10, K], [240, 320, K] and  [480, 640, K]\n",
    "  'imageWidth' : 42, # there is a default Cov layer for PPO with 240 x 320\n",
    "  'imageHeight' : 42,\n",
    "  'trackChoice' : 3, # 1 is High Speed Ring, 2 is 0-400m in MR2, #3 is 0-400m in Supra, #4 is test ring\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debugAsGym:\n",
    "    env = gymnasium.make(\"real-time-gym-v1\", config=my_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debugAsGym:\n",
    "    env.reset()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register the environment in a way that RLlib is happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not debugAsGym and not testResult:\n",
    "    def env_creator(env_config):\n",
    "        env = gymnasium.make(\"real-time-gym-v1\", config=env_config)\n",
    "        return env  # return an env instance\n",
    "\n",
    "    from ray.tune.registry import register_env\n",
    "    register_env(\"gt-rtgym-env-v1\", lambda config: env_creator(my_config)) # better way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-25 22:43:24,115\tINFO worker.py:1612 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "if not debugAsGym and not testResult:\n",
    "    import ray\n",
    "    ray.shutdown()\n",
    "    ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nadir\\anaconda3\\envs\\GTRay2\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py:484: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "c:\\Users\\nadir\\anaconda3\\envs\\GTRay2\\lib\\site-packages\\ray\\tune\\logger\\unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "c:\\Users\\nadir\\anaconda3\\envs\\GTRay2\\lib\\site-packages\\ray\\tune\\logger\\unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "c:\\Users\\nadir\\anaconda3\\envs\\GTRay2\\lib\\site-packages\\ray\\tune\\logger\\unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT Real Time instantiated\n",
      "GT AI Server instantiated for rtgym\n",
      "still simple reward system\n",
      "starting up on localhost port 9999\n",
      "Waiting for a connection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-25 22:43:56,094\tWARNING deprecation.py:50 -- DeprecationWarning: `_build_eager_tf_policy` has been deprecated. This will raise an error in the future!\n",
      "2023-09-25 22:43:56,094\tWARNING deprecation.py:50 -- DeprecationWarning: `EagerTFPolicy` has been deprecated. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection from ('127.0.0.1', 55554)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-25 22:43:56,539\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.tf.tf_modelv2.TFModelV2` has been deprecated. Use `ray.rllib.core.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "2023-09-25 22:43:56,539\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.tf.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "2023-09-25 22:43:56,547\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.tf.misc.normc_initializer` has been deprecated. This will raise an error in the future!\n",
      "2023-09-25 22:43:56,595\tWARNING deprecation.py:50 -- DeprecationWarning: `rllib.models.tf.ComplexInputNetwork` has been deprecated. This will raise an error in the future!\n",
      "2023-09-25 22:43:59,835\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "2023-09-25 22:43:59,835\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "2023-09-25 22:43:59,840\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "2023-09-25 22:44:00,661\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.tf.tf_action_dist.SquashedGaussian` has been deprecated. This will raise an error in the future!\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mj:\\git\\TensorFlowPSX\\Py\\rrlib_experiments_SAC_mode4_Supra_Drag_TF2.ipynb Cell 9\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/j%3A/git/TensorFlowPSX/Py/rrlib_experiments_SAC_mode4_Supra_Drag_TF2.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mray\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrllib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39malgorithms\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mppo\u001b[39;00m \u001b[39mimport\u001b[39;00m PPOConfig\n\u001b[0;32m      <a href='vscode-notebook-cell:/j%3A/git/TensorFlowPSX/Py/rrlib_experiments_SAC_mode4_Supra_Drag_TF2.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mray\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrllib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39malgorithms\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39ma2c\u001b[39;00m \u001b[39mimport\u001b[39;00m A2CConfig\n\u001b[0;32m      <a href='vscode-notebook-cell:/j%3A/git/TensorFlowPSX/Py/rrlib_experiments_SAC_mode4_Supra_Drag_TF2.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m algo \u001b[39m=\u001b[39m (\n\u001b[0;32m      <a href='vscode-notebook-cell:/j%3A/git/TensorFlowPSX/Py/rrlib_experiments_SAC_mode4_Supra_Drag_TF2.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     SACConfig()\n\u001b[0;32m      <a href='vscode-notebook-cell:/j%3A/git/TensorFlowPSX/Py/rrlib_experiments_SAC_mode4_Supra_Drag_TF2.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m.\u001b[39;49mresources(\n\u001b[0;32m      <a href='vscode-notebook-cell:/j%3A/git/TensorFlowPSX/Py/rrlib_experiments_SAC_mode4_Supra_Drag_TF2.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         num_gpus\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/j%3A/git/TensorFlowPSX/Py/rrlib_experiments_SAC_mode4_Supra_Drag_TF2.ipynb#X11sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         num_gpus_per_learner_worker\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/j%3A/git/TensorFlowPSX/Py/rrlib_experiments_SAC_mode4_Supra_Drag_TF2.ipynb#X11sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         )\n\u001b[0;32m     <a href='vscode-notebook-cell:/j%3A/git/TensorFlowPSX/Py/rrlib_experiments_SAC_mode4_Supra_Drag_TF2.ipynb#X11sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m.\u001b[39;49mrollouts(\n\u001b[0;32m     <a href='vscode-notebook-cell:/j%3A/git/TensorFlowPSX/Py/rrlib_experiments_SAC_mode4_Supra_Drag_TF2.ipynb#X11sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         \u001b[39m#num_rollout_workers=1,\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/j%3A/git/TensorFlowPSX/Py/rrlib_experiments_SAC_mode4_Supra_Drag_TF2.ipynb#X11sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         enable_connectors\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/j%3A/git/TensorFlowPSX/Py/rrlib_experiments_SAC_mode4_Supra_Drag_TF2.ipynb#X11sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m         batch_mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtruncate_episodes\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/j%3A/git/TensorFlowPSX/Py/rrlib_experiments_SAC_mode4_Supra_Drag_TF2.ipynb#X11sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m         \u001b[39m#num_envs_per_worker=1\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/j%3A/git/TensorFlowPSX/Py/rrlib_experiments_SAC_mode4_Supra_Drag_TF2.ipynb#X11sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m         \u001b[39m#rollout_fragment_length=256\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/j%3A/git/TensorFlowPSX/Py/rrlib_experiments_SAC_mode4_Supra_Drag_TF2.ipynb#X11sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m         )\n\u001b[0;32m     <a href='vscode-notebook-cell:/j%3A/git/TensorFlowPSX/Py/rrlib_experiments_SAC_mode4_Supra_Drag_TF2.ipynb#X11sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39m.\u001b[39;49mframework(\n\u001b[0;32m     <a href='vscode-notebook-cell:/j%3A/git/TensorFlowPSX/Py/rrlib_experiments_SAC_mode4_Supra_Drag_TF2.ipynb#X11sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m         framework\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtf2\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/j%3A/git/TensorFlowPSX/Py/rrlib_experiments_SAC_mode4_Supra_Drag_TF2.ipynb#X11sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m         \u001b[39m#eager_tracing=True,\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/j%3A/git/TensorFlowPSX/Py/rrlib_experiments_SAC_mode4_Supra_Drag_TF2.ipynb#X11sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m         )\n\u001b[0;32m     <a href='vscode-notebook-cell:/j%3A/git/TensorFlowPSX/Py/rrlib_experiments_SAC_mode4_Supra_Drag_TF2.ipynb#X11sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39m.\u001b[39;49menvironment(\n\u001b[0;32m     <a href='vscode-notebook-cell:/j%3A/git/TensorFlowPSX/Py/rrlib_experiments_SAC_mode4_Supra_Drag_TF2.ipynb#X11sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m         env\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgt-rtgym-env-v1\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/j%3A/git/TensorFlowPSX/Py/rrlib_experiments_SAC_mode4_Supra_Drag_TF2.ipynb#X11sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m         disable_env_checking\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/j%3A/git/TensorFlowPSX/Py/rrlib_experiments_SAC_mode4_Supra_Drag_TF2.ipynb#X11sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m         render_env\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/j%3A/git/TensorFlowPSX/Py/rrlib_experiments_SAC_mode4_Supra_Drag_TF2.ipynb#X11sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m         )\n\u001b[0;32m     <a href='vscode-notebook-cell:/j%3A/git/TensorFlowPSX/Py/rrlib_experiments_SAC_mode4_Supra_Drag_TF2.ipynb#X11sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     \u001b[39m.\u001b[39;49mtraining(\n\u001b[0;32m     <a href='vscode-notebook-cell:/j%3A/git/TensorFlowPSX/Py/rrlib_experiments_SAC_mode4_Supra_Drag_TF2.ipynb#X11sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m         \u001b[39m#lr=tune.grid_search([0.01, 0.001, 0.0001])\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/j%3A/git/TensorFlowPSX/Py/rrlib_experiments_SAC_mode4_Supra_Drag_TF2.ipynb#X11sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m         \u001b[39m#lambda_=0.95,\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/j%3A/git/TensorFlowPSX/Py/rrlib_experiments_SAC_mode4_Supra_Drag_TF2.ipynb#X11sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m         \u001b[39m#gamma=0.99,\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/j%3A/git/TensorFlowPSX/Py/rrlib_experiments_SAC_mode4_Supra_Drag_TF2.ipynb#X11sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m         \u001b[39m#sgd_minibatch_size=128,\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/j%3A/git/TensorFlowPSX/Py/rrlib_experiments_SAC_mode4_Supra_Drag_TF2.ipynb#X11sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m         train_batch_size\u001b[39m=\u001b[39;49m\u001b[39m155\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/j%3A/git/TensorFlowPSX/Py/rrlib_experiments_SAC_mode4_Supra_Drag_TF2.ipynb#X11sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m         \u001b[39m#num_sgd_iter=8,\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/j%3A/git/TensorFlowPSX/Py/rrlib_experiments_SAC_mode4_Supra_Drag_TF2.ipynb#X11sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m         \u001b[39m#clip_param=0.2,\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/j%3A/git/TensorFlowPSX/Py/rrlib_experiments_SAC_mode4_Supra_Drag_TF2.ipynb#X11sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m         \u001b[39m#model={\"fcnet_hiddens\": [1, 8]},\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/j%3A/git/TensorFlowPSX/Py/rrlib_experiments_SAC_mode4_Supra_Drag_TF2.ipynb#X11sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     )\n\u001b[1;32m---> <a href='vscode-notebook-cell:/j%3A/git/TensorFlowPSX/Py/rrlib_experiments_SAC_mode4_Supra_Drag_TF2.ipynb#X11sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     \u001b[39m.\u001b[39;49mbuild()\n\u001b[0;32m     <a href='vscode-notebook-cell:/j%3A/git/TensorFlowPSX/Py/rrlib_experiments_SAC_mode4_Supra_Drag_TF2.ipynb#X11sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\nadir\\anaconda3\\envs\\GTRay2\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm_config.py:1092\u001b[0m, in \u001b[0;36mAlgorithmConfig.build\u001b[1;34m(self, env, logger_creator, use_copy)\u001b[0m\n\u001b[0;32m   1089\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39malgo_class, \u001b[39mstr\u001b[39m):\n\u001b[0;32m   1090\u001b[0m     algo_class \u001b[39m=\u001b[39m get_trainable_cls(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39malgo_class)\n\u001b[1;32m-> 1092\u001b[0m \u001b[39mreturn\u001b[39;00m algo_class(\n\u001b[0;32m   1093\u001b[0m     config\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m use_copy \u001b[39melse\u001b[39;49;00m copy\u001b[39m.\u001b[39;49mdeepcopy(\u001b[39mself\u001b[39;49m),\n\u001b[0;32m   1094\u001b[0m     logger_creator\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlogger_creator,\n\u001b[0;32m   1095\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\nadir\\anaconda3\\envs\\GTRay2\\lib\\site-packages\\ray\\rllib\\algorithms\\sac\\sac.py:354\u001b[0m, in \u001b[0;36mSAC.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    353\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_allow_unknown_subkeys \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mpolicy_model_config\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mq_model_config\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m--> 354\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nadir\\anaconda3\\envs\\GTRay2\\lib\\site-packages\\ray\\rllib\\utils\\deprecation.py:106\u001b[0m, in \u001b[0;36mDeprecated.<locals>._inner.<locals>.patched_init\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[39mif\u001b[39;00m log_once(old \u001b[39mor\u001b[39;00m obj\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m):\n\u001b[0;32m    100\u001b[0m     deprecation_warning(\n\u001b[0;32m    101\u001b[0m         old\u001b[39m=\u001b[39mold \u001b[39mor\u001b[39;00m obj\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m,\n\u001b[0;32m    102\u001b[0m         new\u001b[39m=\u001b[39mnew,\n\u001b[0;32m    103\u001b[0m         help\u001b[39m=\u001b[39mhelp,\n\u001b[0;32m    104\u001b[0m         error\u001b[39m=\u001b[39merror,\n\u001b[0;32m    105\u001b[0m     )\n\u001b[1;32m--> 106\u001b[0m \u001b[39mreturn\u001b[39;00m obj_init(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nadir\\anaconda3\\envs\\GTRay2\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py:517\u001b[0m, in \u001b[0;36mAlgorithm.__init__\u001b[1;34m(self, config, env, logger_creator, **kwargs)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[39m# Initialize common evaluation_metrics to nan, before they become\u001b[39;00m\n\u001b[0;32m    500\u001b[0m \u001b[39m# available. We want to make sure the metrics are always present\u001b[39;00m\n\u001b[0;32m    501\u001b[0m \u001b[39m# (although their values may be nan), so that Tune does not complain\u001b[39;00m\n\u001b[0;32m    502\u001b[0m \u001b[39m# when we use these as stopping criteria.\u001b[39;00m\n\u001b[0;32m    503\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_metrics \u001b[39m=\u001b[39m {\n\u001b[0;32m    504\u001b[0m     \u001b[39m# TODO: Don't dump sampler results into top-level.\u001b[39;00m\n\u001b[0;32m    505\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mevaluation\u001b[39m\u001b[39m\"\u001b[39m: {\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    514\u001b[0m     },\n\u001b[0;32m    515\u001b[0m }\n\u001b[1;32m--> 517\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[0;32m    518\u001b[0m     config\u001b[39m=\u001b[39mconfig,\n\u001b[0;32m    519\u001b[0m     logger_creator\u001b[39m=\u001b[39mlogger_creator,\n\u001b[0;32m    520\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    521\u001b[0m )\n\u001b[0;32m    523\u001b[0m \u001b[39m# Check, whether `training_iteration` is still a tune.Trainable property\u001b[39;00m\n\u001b[0;32m    524\u001b[0m \u001b[39m# and has not been overridden by the user in the attempt to implement the\u001b[39;00m\n\u001b[0;32m    525\u001b[0m \u001b[39m# algos logic (this should be done now inside `training_step`).\u001b[39;00m\n\u001b[0;32m    526\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\nadir\\anaconda3\\envs\\GTRay2\\lib\\site-packages\\ray\\tune\\trainable\\trainable.py:169\u001b[0m, in \u001b[0;36mTrainable.__init__\u001b[1;34m(self, config, logger_creator, remote_checkpoint_dir, sync_config)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m    168\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_local_ip \u001b[39m=\u001b[39m ray\u001b[39m.\u001b[39mutil\u001b[39m.\u001b[39mget_node_ip_address()\n\u001b[1;32m--> 169\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msetup(copy\u001b[39m.\u001b[39;49mdeepcopy(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig))\n\u001b[0;32m    170\u001b[0m setup_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n\u001b[0;32m    171\u001b[0m \u001b[39mif\u001b[39;00m setup_time \u001b[39m>\u001b[39m SETUP_TIME_THRESHOLD:\n",
      "File \u001b[1;32mc:\\Users\\nadir\\anaconda3\\envs\\GTRay2\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py:639\u001b[0m, in \u001b[0;36mAlgorithm.setup\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[39m# Only if user did not override `_init()`:\u001b[39;00m\n\u001b[0;32m    637\u001b[0m \u001b[39mif\u001b[39;00m _init \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m    638\u001b[0m     \u001b[39m# Create a set of env runner actors via a WorkerSet.\u001b[39;00m\n\u001b[1;32m--> 639\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworkers \u001b[39m=\u001b[39m WorkerSet(\n\u001b[0;32m    640\u001b[0m         env_creator\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv_creator,\n\u001b[0;32m    641\u001b[0m         validate_env\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalidate_env,\n\u001b[0;32m    642\u001b[0m         default_policy_class\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_default_policy_class(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig),\n\u001b[0;32m    643\u001b[0m         config\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig,\n\u001b[0;32m    644\u001b[0m         num_workers\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mnum_rollout_workers,\n\u001b[0;32m    645\u001b[0m         local_worker\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    646\u001b[0m         logdir\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlogdir,\n\u001b[0;32m    647\u001b[0m     )\n\u001b[0;32m    649\u001b[0m     \u001b[39m# TODO (avnishn): Remove the execution plan API by q1 2023\u001b[39;00m\n\u001b[0;32m    650\u001b[0m     \u001b[39m# Function defining one single training iteration's behavior.\u001b[39;00m\n\u001b[0;32m    651\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39m_disable_execution_plan_api:\n\u001b[0;32m    652\u001b[0m         \u001b[39m# Ensure remote workers are initially in sync with the local worker.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nadir\\anaconda3\\envs\\GTRay2\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py:157\u001b[0m, in \u001b[0;36mWorkerSet.__init__\u001b[1;34m(self, env_creator, validate_env, default_policy_class, config, num_workers, local_worker, logdir, _setup)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[39mif\u001b[39;00m _setup:\n\u001b[0;32m    156\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 157\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setup(\n\u001b[0;32m    158\u001b[0m             validate_env\u001b[39m=\u001b[39;49mvalidate_env,\n\u001b[0;32m    159\u001b[0m             config\u001b[39m=\u001b[39;49mconfig,\n\u001b[0;32m    160\u001b[0m             num_workers\u001b[39m=\u001b[39;49mnum_workers,\n\u001b[0;32m    161\u001b[0m             local_worker\u001b[39m=\u001b[39;49mlocal_worker,\n\u001b[0;32m    162\u001b[0m         )\n\u001b[0;32m    163\u001b[0m     \u001b[39m# WorkerSet creation possibly fails, if some (remote) workers cannot\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[39m# be initialized properly (due to some errors in the RolloutWorker's\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[39m# constructor).\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[39mexcept\u001b[39;00m RayActorError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    167\u001b[0m         \u001b[39m# In case of an actor (remote worker) init failure, the remote worker\u001b[39;00m\n\u001b[0;32m    168\u001b[0m         \u001b[39m# may still exist and will be accessible, however, e.g. calling\u001b[39;00m\n\u001b[0;32m    169\u001b[0m         \u001b[39m# its `sample.remote()` would result in strange \"property not found\"\u001b[39;00m\n\u001b[0;32m    170\u001b[0m         \u001b[39m# errors.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nadir\\anaconda3\\envs\\GTRay2\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py:247\u001b[0m, in \u001b[0;36mWorkerSet._setup\u001b[1;34m(self, validate_env, config, num_workers, local_worker)\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[39m# Create a local worker, if needed.\u001b[39;00m\n\u001b[0;32m    246\u001b[0m \u001b[39mif\u001b[39;00m local_worker:\n\u001b[1;32m--> 247\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_local_worker \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_worker(\n\u001b[0;32m    248\u001b[0m         \u001b[39mcls\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv_runner_cls,\n\u001b[0;32m    249\u001b[0m         env_creator\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_env_creator,\n\u001b[0;32m    250\u001b[0m         validate_env\u001b[39m=\u001b[39;49mvalidate_env,\n\u001b[0;32m    251\u001b[0m         worker_index\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[0;32m    252\u001b[0m         num_workers\u001b[39m=\u001b[39;49mnum_workers,\n\u001b[0;32m    253\u001b[0m         config\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_local_config,\n\u001b[0;32m    254\u001b[0m         spaces\u001b[39m=\u001b[39;49mspaces,\n\u001b[0;32m    255\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\nadir\\anaconda3\\envs\\GTRay2\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py:925\u001b[0m, in \u001b[0;36mWorkerSet._make_worker\u001b[1;34m(self, cls, env_creator, validate_env, worker_index, num_workers, recreated_worker, config, spaces)\u001b[0m\n\u001b[0;32m    911\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_make_worker\u001b[39m(\n\u001b[0;32m    912\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    913\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    923\u001b[0m     ] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    924\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[RolloutWorker, ActorHandle]:\n\u001b[1;32m--> 925\u001b[0m     worker \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(\n\u001b[0;32m    926\u001b[0m         env_creator\u001b[39m=\u001b[39;49menv_creator,\n\u001b[0;32m    927\u001b[0m         validate_env\u001b[39m=\u001b[39;49mvalidate_env,\n\u001b[0;32m    928\u001b[0m         default_policy_class\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_policy_class,\n\u001b[0;32m    929\u001b[0m         config\u001b[39m=\u001b[39;49mconfig,\n\u001b[0;32m    930\u001b[0m         worker_index\u001b[39m=\u001b[39;49mworker_index,\n\u001b[0;32m    931\u001b[0m         num_workers\u001b[39m=\u001b[39;49mnum_workers,\n\u001b[0;32m    932\u001b[0m         recreated_worker\u001b[39m=\u001b[39;49mrecreated_worker,\n\u001b[0;32m    933\u001b[0m         log_dir\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_logdir,\n\u001b[0;32m    934\u001b[0m         spaces\u001b[39m=\u001b[39;49mspaces,\n\u001b[0;32m    935\u001b[0m         dataset_shards\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ds_shards,\n\u001b[0;32m    936\u001b[0m     )\n\u001b[0;32m    938\u001b[0m     \u001b[39mreturn\u001b[39;00m worker\n",
      "File \u001b[1;32mc:\\Users\\nadir\\anaconda3\\envs\\GTRay2\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py:525\u001b[0m, in \u001b[0;36mRolloutWorker.__init__\u001b[1;34m(self, env_creator, validate_env, config, worker_index, num_workers, recreated_worker, log_dir, spaces, default_policy_class, dataset_shards, tf_session_creator)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[39m# if RLModule API is enabled, marl_module_spec holds the specs of the RLModules\u001b[39;00m\n\u001b[0;32m    524\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmarl_module_spec \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 525\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_policy_map(policy_dict\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy_dict)\n\u001b[0;32m    527\u001b[0m \u001b[39m# Update Policy's view requirements from Model, only if Policy directly\u001b[39;00m\n\u001b[0;32m    528\u001b[0m \u001b[39m# inherited from base `Policy` class. At this point here, the Policy\u001b[39;00m\n\u001b[0;32m    529\u001b[0m \u001b[39m# must have it's Model (if any) defined and ready to output an initial\u001b[39;00m\n\u001b[0;32m    530\u001b[0m \u001b[39m# state.\u001b[39;00m\n\u001b[0;32m    531\u001b[0m \u001b[39mfor\u001b[39;00m pol \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy_map\u001b[39m.\u001b[39mvalues():\n",
      "File \u001b[1;32mc:\\Users\\nadir\\anaconda3\\envs\\GTRay2\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py:1727\u001b[0m, in \u001b[0;36mRolloutWorker._update_policy_map\u001b[1;34m(self, policy_dict, policy, policy_states, single_agent_rl_module_spec)\u001b[0m\n\u001b[0;32m   1722\u001b[0m     updated_policy_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_policy_dict_with_marl_module(\n\u001b[0;32m   1723\u001b[0m         updated_policy_dict\n\u001b[0;32m   1724\u001b[0m     )\n\u001b[0;32m   1726\u001b[0m \u001b[39m# Builds the self.policy_map dict\u001b[39;00m\n\u001b[1;32m-> 1727\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_build_policy_map(\n\u001b[0;32m   1728\u001b[0m     policy_dict\u001b[39m=\u001b[39;49mupdated_policy_dict,\n\u001b[0;32m   1729\u001b[0m     policy\u001b[39m=\u001b[39;49mpolicy,\n\u001b[0;32m   1730\u001b[0m     policy_states\u001b[39m=\u001b[39;49mpolicy_states,\n\u001b[0;32m   1731\u001b[0m )\n\u001b[0;32m   1733\u001b[0m \u001b[39m# Initialize the filter dict\u001b[39;00m\n\u001b[0;32m   1734\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_filter_dict(updated_policy_dict)\n",
      "File \u001b[1;32mc:\\Users\\nadir\\anaconda3\\envs\\GTRay2\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py:1838\u001b[0m, in \u001b[0;36mRolloutWorker._build_policy_map\u001b[1;34m(self, policy_dict, policy, policy_states)\u001b[0m\n\u001b[0;32m   1835\u001b[0m \u001b[39mfor\u001b[39;00m name, policy_spec \u001b[39min\u001b[39;00m \u001b[39msorted\u001b[39m(policy_dict\u001b[39m.\u001b[39mitems()):\n\u001b[0;32m   1836\u001b[0m     \u001b[39m# Create the actual policy object.\u001b[39;00m\n\u001b[0;32m   1837\u001b[0m     \u001b[39mif\u001b[39;00m policy \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1838\u001b[0m         new_policy \u001b[39m=\u001b[39m create_policy_for_framework(\n\u001b[0;32m   1839\u001b[0m             policy_id\u001b[39m=\u001b[39;49mname,\n\u001b[0;32m   1840\u001b[0m             policy_class\u001b[39m=\u001b[39;49mget_tf_eager_cls_if_necessary(\n\u001b[0;32m   1841\u001b[0m                 policy_spec\u001b[39m.\u001b[39;49mpolicy_class, policy_spec\u001b[39m.\u001b[39;49mconfig\n\u001b[0;32m   1842\u001b[0m             ),\n\u001b[0;32m   1843\u001b[0m             merged_config\u001b[39m=\u001b[39;49mpolicy_spec\u001b[39m.\u001b[39;49mconfig,\n\u001b[0;32m   1844\u001b[0m             observation_space\u001b[39m=\u001b[39;49mpolicy_spec\u001b[39m.\u001b[39;49mobservation_space,\n\u001b[0;32m   1845\u001b[0m             action_space\u001b[39m=\u001b[39;49mpolicy_spec\u001b[39m.\u001b[39;49maction_space,\n\u001b[0;32m   1846\u001b[0m             worker_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mworker_index,\n\u001b[0;32m   1847\u001b[0m             seed\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseed,\n\u001b[0;32m   1848\u001b[0m         )\n\u001b[0;32m   1849\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1850\u001b[0m         new_policy \u001b[39m=\u001b[39m policy\n",
      "File \u001b[1;32mc:\\Users\\nadir\\anaconda3\\envs\\GTRay2\\lib\\site-packages\\ray\\rllib\\utils\\policy.py:139\u001b[0m, in \u001b[0;36mcreate_policy_for_framework\u001b[1;34m(policy_id, policy_class, merged_config, observation_space, action_space, worker_index, session_creator, seed)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[39m# For tf-eager: no graph, no session.\u001b[39;00m\n\u001b[0;32m    137\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    138\u001b[0m         \u001b[39mwith\u001b[39;00m tf1\u001b[39m.\u001b[39mvariable_scope(var_scope):\n\u001b[1;32m--> 139\u001b[0m             \u001b[39mreturn\u001b[39;00m policy_class(observation_space, action_space, merged_config)\n\u001b[0;32m    140\u001b[0m \u001b[39m# Non-tf: No graph, no session.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m policy_class(observation_space, action_space, merged_config)\n",
      "File \u001b[1;32mc:\\Users\\nadir\\anaconda3\\envs\\GTRay2\\lib\\site-packages\\ray\\rllib\\policy\\eager_tf_policy.py:168\u001b[0m, in \u001b[0;36m_traced_eager_policy.<locals>.TracedEagerPolicy.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_traced_compute_gradients_helper \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_traced_apply_gradients_helper \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m--> 168\u001b[0m \u001b[39msuper\u001b[39m(TracedEagerPolicy, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nadir\\anaconda3\\envs\\GTRay2\\lib\\site-packages\\ray\\rllib\\policy\\eager_tf_policy.py:480\u001b[0m, in \u001b[0;36m_build_eager_tf_policy.<locals>.eager_policy_cls.__init__\u001b[1;34m(self, observation_space, action_space, config)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[39m# Backward compatibility: A user's policy may only support a single\u001b[39;00m\n\u001b[0;32m    477\u001b[0m \u001b[39m# loss term and optimizer (no lists).\u001b[39;00m\n\u001b[0;32m    478\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer: LocalOptimizer \u001b[39m=\u001b[39m optimizers[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m optimizers \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 480\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_initialize_loss_from_dummy_batch(\n\u001b[0;32m    481\u001b[0m     auto_remove_unneeded_view_reqs\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    482\u001b[0m     stats_fn\u001b[39m=\u001b[39;49mstats_fn,\n\u001b[0;32m    483\u001b[0m )\n\u001b[0;32m    484\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_loss_initialized \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    486\u001b[0m \u001b[39mif\u001b[39;00m after_init:\n",
      "File \u001b[1;32mc:\\Users\\nadir\\anaconda3\\envs\\GTRay2\\lib\\site-packages\\ray\\rllib\\policy\\policy.py:1418\u001b[0m, in \u001b[0;36mPolicy._initialize_loss_from_dummy_batch\u001b[1;34m(self, auto_remove_unneeded_view_reqs, stats_fn)\u001b[0m\n\u001b[0;32m   1414\u001b[0m \u001b[39m# With RL Modules you want the explore flag to be True for initialization\u001b[39;00m\n\u001b[0;32m   1415\u001b[0m \u001b[39m# of the tensors and placeholder you'd need for training.\u001b[39;00m\n\u001b[0;32m   1416\u001b[0m explore \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39m_enable_rl_module_api\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m-> 1418\u001b[0m actions, state_outs, extra_outs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_actions_from_input_dict(\n\u001b[0;32m   1419\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dummy_batch, explore\u001b[39m=\u001b[39;49mexplore\n\u001b[0;32m   1420\u001b[0m )\n\u001b[0;32m   1421\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39m_enable_rl_module_api\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m   1422\u001b[0m     \u001b[39mfor\u001b[39;00m key, view_req \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mview_requirements\u001b[39m.\u001b[39mitems():\n",
      "File \u001b[1;32mc:\\Users\\nadir\\anaconda3\\envs\\GTRay2\\lib\\site-packages\\ray\\rllib\\policy\\eager_tf_policy.py:143\u001b[0m, in \u001b[0;36m_check_too_many_retraces.<locals>._func\u001b[1;34m(self_, *args, **kwargs)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    133\u001b[0m     self_\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39meager_max_retraces\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    134\u001b[0m     \u001b[39mand\u001b[39;00m self_\u001b[39m.\u001b[39m_re_trace_counter \u001b[39m>\u001b[39m self_\u001b[39m.\u001b[39mconfig[\u001b[39m\"\u001b[39m\u001b[39meager_max_retraces\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    135\u001b[0m ):\n\u001b[0;32m    136\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    137\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mToo many tf-eager re-traces detected! This could lead to\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    138\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m significant slow-downs (even slower than running in \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    141\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m in your config to None.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    142\u001b[0m     )\n\u001b[1;32m--> 143\u001b[0m \u001b[39mreturn\u001b[39;00m obj(self_, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nadir\\anaconda3\\envs\\GTRay2\\lib\\site-packages\\ray\\rllib\\policy\\eager_tf_policy.py:219\u001b[0m, in \u001b[0;36m_traced_eager_policy.<locals>.TracedEagerPolicy.compute_actions_from_input_dict\u001b[1;34m(self, input_dict, explore, timestep, episodes, **kwargs)\u001b[0m\n\u001b[0;32m    215\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_traced_compute_actions_helper \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    217\u001b[0m \u001b[39m# Now that the helper method is traced, call super's\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[39m# `compute_actions_from_input_dict()` (which will call the traced helper).\u001b[39;00m\n\u001b[1;32m--> 219\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m(TracedEagerPolicy, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mcompute_actions_from_input_dict(\n\u001b[0;32m    220\u001b[0m     input_dict\u001b[39m=\u001b[39minput_dict,\n\u001b[0;32m    221\u001b[0m     explore\u001b[39m=\u001b[39mexplore,\n\u001b[0;32m    222\u001b[0m     timestep\u001b[39m=\u001b[39mtimestep,\n\u001b[0;32m    223\u001b[0m     episodes\u001b[39m=\u001b[39mepisodes,\n\u001b[0;32m    224\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    225\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\nadir\\anaconda3\\envs\\GTRay2\\lib\\site-packages\\ray\\rllib\\policy\\eager_tf_policy.py:528\u001b[0m, in \u001b[0;36m_build_eager_tf_policy.<locals>.eager_policy_cls.compute_actions_from_input_dict\u001b[1;34m(self, input_dict, explore, timestep, episodes, **kwargs)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[39m# Call the exploration before_compute_actions hook.\u001b[39;00m\n\u001b[0;32m    524\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexploration\u001b[39m.\u001b[39mbefore_compute_actions(\n\u001b[0;32m    525\u001b[0m     timestep\u001b[39m=\u001b[39mtimestep, explore\u001b[39m=\u001b[39mexplore, tf_sess\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_session()\n\u001b[0;32m    526\u001b[0m )\n\u001b[1;32m--> 528\u001b[0m ret \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compute_actions_helper(\n\u001b[0;32m    529\u001b[0m     input_dict,\n\u001b[0;32m    530\u001b[0m     state_batches,\n\u001b[0;32m    531\u001b[0m     \u001b[39m# TODO: Passing episodes into a traced method does not work.\u001b[39;49;00m\n\u001b[0;32m    532\u001b[0m     \u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39meager_tracing\u001b[39;49m\u001b[39m\"\u001b[39;49m] \u001b[39melse\u001b[39;49;00m episodes,\n\u001b[0;32m    533\u001b[0m     explore,\n\u001b[0;32m    534\u001b[0m     timestep,\n\u001b[0;32m    535\u001b[0m )\n\u001b[0;32m    536\u001b[0m \u001b[39m# Update our global timestep by the batch size.\u001b[39;00m\n\u001b[0;32m    537\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mglobal_timestep\u001b[39m.\u001b[39massign_add(tree\u001b[39m.\u001b[39mflatten(ret[\u001b[39m0\u001b[39m])[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape\u001b[39m.\u001b[39mas_list()[\u001b[39m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\nadir\\anaconda3\\envs\\GTRay2\\lib\\site-packages\\ray\\rllib\\utils\\threading.py:24\u001b[0m, in \u001b[0;36mwith_lock.<locals>.wrapper\u001b[1;34m(self, *a, **k)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     23\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m---> 24\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mk)\n\u001b[0;32m     25\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     26\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mhas no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m_lock\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m e\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\nadir\\anaconda3\\envs\\GTRay2\\lib\\site-packages\\ray\\rllib\\policy\\eager_tf_policy.py:937\u001b[0m, in \u001b[0;36m_build_eager_tf_policy.<locals>.eager_policy_cls._compute_actions_helper\u001b[1;34m(self, input_dict, state_batches, episodes, explore, timestep)\u001b[0m\n\u001b[0;32m    932\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    933\u001b[0m     dist_inputs, state_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(\n\u001b[0;32m    934\u001b[0m         input_dict, state_batches, seq_lens\n\u001b[0;32m    935\u001b[0m     )\n\u001b[1;32m--> 937\u001b[0m action_dist \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdist_class(dist_inputs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel)\n\u001b[0;32m    939\u001b[0m \u001b[39m# Get the exploration action from the forward results.\u001b[39;00m\n\u001b[0;32m    940\u001b[0m actions, logp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexploration\u001b[39m.\u001b[39mget_exploration_action(\n\u001b[0;32m    941\u001b[0m     action_distribution\u001b[39m=\u001b[39maction_dist,\n\u001b[0;32m    942\u001b[0m     timestep\u001b[39m=\u001b[39mtimestep,\n\u001b[0;32m    943\u001b[0m     explore\u001b[39m=\u001b[39mexplore,\n\u001b[0;32m    944\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\nadir\\anaconda3\\envs\\GTRay2\\lib\\site-packages\\ray\\rllib\\models\\tf\\tf_action_dist.py:457\u001b[0m, in \u001b[0;36mSquashedGaussian.__init__\u001b[1;34m(self, inputs, model, low, high)\u001b[0m\n\u001b[0;32m    453\u001b[0m \u001b[39mif\u001b[39;00m log_once(\u001b[39m\"\u001b[39m\u001b[39mtf_action_dist_squashed_gaussian_deprecation\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    454\u001b[0m     deprecation_warning(\n\u001b[0;32m    455\u001b[0m         old\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mray.rllib.models.tf.tf_action_dist.SquashedGaussian\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    456\u001b[0m     )\n\u001b[1;32m--> 457\u001b[0m \u001b[39massert\u001b[39;00m tfp \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    458\u001b[0m mean, log_std \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39msplit(inputs, \u001b[39m2\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    459\u001b[0m \u001b[39m# Clip `scale` values (coming from NN) to reasonable values.\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if not debugAsGym and not testResult:\n",
    "    from ray.rllib.algorithms.sac import SACConfig\n",
    "    from ray.rllib.algorithms.ppo import PPOConfig\n",
    "    from ray.rllib.algorithms.a2c import A2CConfig\n",
    "\n",
    "    algo = (\n",
    "        SACConfig()\n",
    "        .resources(\n",
    "            num_gpus=1,\n",
    "            num_gpus_per_learner_worker=1,\n",
    "            )\n",
    "        .rollouts(\n",
    "            #num_rollout_workers=1,\n",
    "            enable_connectors=True,\n",
    "            batch_mode=\"truncate_episodes\",\n",
    "            #num_envs_per_worker=1\n",
    "            #rollout_fragment_length=256\n",
    "            )\n",
    "        .framework(\n",
    "            framework=\"tf2\",\n",
    "            #eager_tracing=True,\n",
    "            )\n",
    "        .environment(\n",
    "            env=\"gt-rtgym-env-v1\",\n",
    "            disable_env_checking=True,\n",
    "            render_env=False,\n",
    "            )\n",
    "        .training(\n",
    "            #lr=tune.grid_search([0.01, 0.001, 0.0001])\n",
    "            #lambda_=0.95,\n",
    "            #gamma=0.99,\n",
    "            #sgd_minibatch_size=128,\n",
    "            train_batch_size=155,\n",
    "            #num_sgd_iter=8,\n",
    "            #clip_param=0.2,\n",
    "            #model={\"fcnet_hiddens\": [1, 8]},\n",
    "        )\n",
    "        .build()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not debugAsGym and not testResult:\n",
    "    N = 1000\n",
    "\n",
    "    for n in range(N):\n",
    "        result = algo.train()\n",
    "        print(\"Loop: \", n)\n",
    "        if n % 10 == 0:\n",
    "            print(\"Saved\", n)\n",
    "            algo.save()\n",
    "            \n",
    "    algo.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not debugAsGym and not testResult:\n",
    "    N = 1000\n",
    "\n",
    "    for n in range(N):\n",
    "        result = algo.train()\n",
    "        print(\"Loop: \", n)\n",
    "        if n % 50 == 0:\n",
    "            print(\"Saved\", n)\n",
    "            algo.save()\n",
    "            \n",
    "    algo.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from myRTClass import MyGranTurismoRTGYM, DEFAULT_CONFIG_DICT\n",
    "import gymnasium\n",
    "\n",
    "my_config = DEFAULT_CONFIG_DICT\n",
    "my_config[\"interface\"] = MyGranTurismoRTGYM\n",
    "my_config[\"time_step_duration\"] = 0.1\n",
    "my_config[\"start_obs_capture\"] = 0.1\n",
    "my_config[\"time_step_timeout_factor\"] = 1.0\n",
    "#my_config[\"ep_max_length\"] = 224\n",
    "my_config[\"act_buf_len\"] = 3\n",
    "my_config[\"reset_act_buf\"] = False\n",
    "my_config[\"benchmark\"] = True\n",
    "my_config[\"benchmark_polyak\"] = 0.2\n",
    "\n",
    "my_config[\"interface_kwargs\"] = {\n",
    "  'debugFlag': False, # do not use render() while True\n",
    "  'img_hist_len': 3,\n",
    "  'modelMode': 4,\n",
    "  'agent' : 'PPO',\n",
    "  #  [42, 42, K], [84, 84, K], [10, 10, K], [240, 320, K] and  [480, 640, K]\n",
    "  'imageWidth' : 42, # there is a default Cov layer for PPO with 240 x 320\n",
    "  'imageHeight' : 42,\n",
    "  'trackChoice' : 3, # 1 is High Speed Ring, 2 is 0-400m, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not debugAsGym and testResult:\n",
    "    def env_creator(env_config):\n",
    "        env = gymnasium.make(\"real-time-gym-v1\", config=env_config)\n",
    "        return env  # return an env instance\n",
    "\n",
    "    from ray.tune.registry import register_env\n",
    "    register_env(\"gt-rtgym-env-v1\", lambda config: env_creator(my_config)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not debugAsGym and testResult:\n",
    "    from ray.rllib.algorithms.algorithm import Algorithm\n",
    "    algo = Algorithm.from_checkpoint(\"C:/Users/nadir/ray_results/PPO_gt-rtgym-env-v1_2023-05-19_07-37-37z3d6v2w2/checkpoint_000061\")\n",
    "    #algo = Algorithm.from_checkpoint(\"C:/Users/nadir/ray_results/PPO_gt-rtgym-env-v1_2023-05-19_07-37-37z3d6v2w2/checkpoint_002000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not debugAsGym and testResult:\n",
    "    result = algo.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not debugAsGym and testResult:\n",
    "\n",
    "    policy = algo.get_policy()\n",
    "    #print(policy.model)\n",
    "    model = policy.model\n",
    "    print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GTAI2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
