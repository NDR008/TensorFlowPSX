{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Gym Environment\n",
    "\n",
    "```py\n",
    "my_config[\"interface\"] = MyGranTurismoRTGYM\n",
    "my_config[\"time_step_duration\"] = 0.05 # when to give up\n",
    "my_config[\"start_obs_capture\"] = 0.05 # when to capture \n",
    "my_config[\"time_step_timeout_factor\"] = 1.0 # how late is OK\n",
    "my_config[\"act_buf_len\"] = 3 # how many past actions\n",
    "my_config[\"reset_act_buf\"] = True # resect action buffer on reset\n",
    "my_config[\"benchmark\"] = True\n",
    "my_config[\"benchmark_polyak\"] = 0.2\n",
    "```\n",
    "\n",
    "This section needs to be setup for any method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    print(\"GPU is not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugAsGym = False\n",
    "testResult = False\n",
    "\n",
    "from myRTClass import MyGranTurismoRTGYM, DEFAULT_CONFIG_DICT\n",
    "import gymnasium\n",
    "\n",
    "my_config = DEFAULT_CONFIG_DICT\n",
    "my_config[\"interface\"] = MyGranTurismoRTGYM\n",
    "my_config[\"time_step_duration\"] = 0.1\n",
    "my_config[\"start_obs_capture\"] = 0.1\n",
    "my_config[\"time_step_timeout_factor\"] = 1.0\n",
    "my_config[\"ep_max_length\"] = 224\n",
    "my_config[\"act_buf_len\"] = 3\n",
    "my_config[\"reset_act_buf\"] = False\n",
    "my_config[\"benchmark\"] = True\n",
    "my_config[\"benchmark_polyak\"] = 0.2\n",
    "\n",
    "my_config[\"interface_kwargs\"] = {\n",
    "  'debugFlag': False, # do not use render() while True\n",
    "  'img_hist_len': 3,\n",
    "  'modelMode': 2,\n",
    "  'agent' : 'PPO',\n",
    "  #  [42, 42, K], [84, 84, K], [10, 10, K], [240, 320, K] and  [480, 640, K]\n",
    "  'imageWidth' : 42, # there is a default Cov layer for PPO with 240 x 320\n",
    "  'imageHeight' : 42,\n",
    "  'trackChoice' : 3, # 1 is High Speed Ring, 2 is 0-400m, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debugAsGym:\n",
    "    env = gymnasium.make(\"real-time-gym-v1\", config=my_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debugAsGym:\n",
    "    env.reset()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register the environment in a way that RLlib is happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not debugAsGym and not testResult:\n",
    "    def env_creator(env_config):\n",
    "        env = gymnasium.make(\"real-time-gym-v1\", config=env_config)\n",
    "        return env  # return an env instance\n",
    "\n",
    "    from ray.tune.registry import register_env\n",
    "    register_env(\"gt-rtgym-env-v1\", lambda config: env_creator(my_config)) # better way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-25 23:27:11,663\tINFO worker.py:1616 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "if not debugAsGym and not testResult:\n",
    "    import ray\n",
    "    ray.shutdown()\n",
    "    ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-25 23:27:15,244\tINFO algorithm_config.py:3307 -- Executing eagerly (framework='tf2'), with eager_tracing=False. For production workloads, make sure to set eager_tracing=True  in order to match the speed of tf-static-graph (framework='tf'). For debugging purposes, `eager_tracing=False` is the best choice.\n",
      "2023-09-25 23:27:15,289\tINFO algorithm.py:527 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=12964)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=12964)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12964)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=12964)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12964)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=12964)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12964)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=12964)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12964)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=12964)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12964)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=12964)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12964)\u001b[0m WARNING:tensorflow:From c:\\Users\\nadir\\anaconda3\\envs\\GTRay2.4_torch_tf\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:561: calling function (from tensorflow.python.eager.def_function) with experimental_relax_shapes is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=12964)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=12964)\u001b[0m experimental_relax_shapes is deprecated, use reduce_retracing instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=12964)\u001b[0m GT Real Time instantiated\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=12964)\u001b[0m GT AI Server instantiated for rtgym\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=12964)\u001b[0m still simple reward system\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=12964)\u001b[0m starting up on localhost port 9999\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=12964)\u001b[0m Waiting for a connection\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=12964)\u001b[0m Connection from ('127.0.0.1', 57443)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-25 23:27:33,544\tINFO trainable.py:172 -- Trainable.setup took 18.260 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    }
   ],
   "source": [
    "if not debugAsGym and not testResult:\n",
    "    from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "    algo = (\n",
    "        PPOConfig()\n",
    "        .resources(\n",
    "            num_gpus=1\n",
    "            )\n",
    "        .rollouts(\n",
    "            num_rollout_workers=1,\n",
    "            #enable_connectors=True,\n",
    "            batch_mode=\"truncate_episodes\",\n",
    "            )\n",
    "        .framework(\n",
    "            framework=\"tf2\",\n",
    "            )\n",
    "        .environment(\n",
    "            env=\"gt-rtgym-env-v1\",\n",
    "            disable_env_checking=True,\n",
    "            render_env=False,\n",
    "            )\n",
    "        \n",
    "        .training(\n",
    "            train_batch_size=155,\n",
    "        )\n",
    "        .build()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=12964)\u001b[0m reset triggered\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=12964)\u001b[0m reload save for track : 4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mj:\\git\\TensorFlowPSX\\Py\\rrlib_experiments_PPO_mode4_Supra_Drag_TF2.ipynb Cell 11\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/j%3A/git/TensorFlowPSX/Py/rrlib_experiments_PPO_mode4_Supra_Drag_TF2.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m N \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/j%3A/git/TensorFlowPSX/Py/rrlib_experiments_PPO_mode4_Supra_Drag_TF2.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(N):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/j%3A/git/TensorFlowPSX/Py/rrlib_experiments_PPO_mode4_Supra_Drag_TF2.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     result \u001b[39m=\u001b[39m algo\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m      <a href='vscode-notebook-cell:/j%3A/git/TensorFlowPSX/Py/rrlib_experiments_PPO_mode4_Supra_Drag_TF2.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLoop: \u001b[39m\u001b[39m\"\u001b[39m, n)\n\u001b[0;32m      <a href='vscode-notebook-cell:/j%3A/git/TensorFlowPSX/Py/rrlib_experiments_PPO_mode4_Supra_Drag_TF2.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mif\u001b[39;00m n \u001b[39m%\u001b[39m \u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\nadir\\anaconda3\\envs\\GTRay2.4_torch_tf\\lib\\site-packages\\ray\\tune\\trainable\\trainable.py:381\u001b[0m, in \u001b[0;36mTrainable.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    379\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m    380\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 381\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m    382\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    383\u001b[0m     skipped \u001b[39m=\u001b[39m skip_exceptions(e)\n",
      "File \u001b[1;32mc:\\Users\\nadir\\anaconda3\\envs\\GTRay2.4_torch_tf\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py:792\u001b[0m, in \u001b[0;36mAlgorithm.step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    784\u001b[0m     (\n\u001b[0;32m    785\u001b[0m         results,\n\u001b[0;32m    786\u001b[0m         train_iter_ctx,\n\u001b[0;32m    787\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_one_training_iteration_and_evaluation_in_parallel()\n\u001b[0;32m    788\u001b[0m \u001b[39m# - No evaluation necessary, just run the next training iteration.\u001b[39;00m\n\u001b[0;32m    789\u001b[0m \u001b[39m# - We have to evaluate in this training iteration, but no parallelism ->\u001b[39;00m\n\u001b[0;32m    790\u001b[0m \u001b[39m#   evaluate after the training iteration is entirely done.\u001b[39;00m\n\u001b[0;32m    791\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 792\u001b[0m     results, train_iter_ctx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_one_training_iteration()\n\u001b[0;32m    794\u001b[0m \u001b[39m# Sequential: Train (already done above), then evaluate.\u001b[39;00m\n\u001b[0;32m    795\u001b[0m \u001b[39mif\u001b[39;00m evaluate_this_iter \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mevaluation_parallel_to_training:\n",
      "File \u001b[1;32mc:\\Users\\nadir\\anaconda3\\envs\\GTRay2.4_torch_tf\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py:2811\u001b[0m, in \u001b[0;36mAlgorithm._run_one_training_iteration\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2809\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timers[TRAINING_ITERATION_TIMER]:\n\u001b[0;32m   2810\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39m_disable_execution_plan_api:\n\u001b[1;32m-> 2811\u001b[0m         results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step()\n\u001b[0;32m   2812\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2813\u001b[0m         results \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_exec_impl)\n",
      "File \u001b[1;32mc:\\Users\\nadir\\anaconda3\\envs\\GTRay2.4_torch_tf\\lib\\site-packages\\ray\\rllib\\algorithms\\ppo\\ppo.py:403\u001b[0m, in \u001b[0;36mPPO.training_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    398\u001b[0m         train_batch \u001b[39m=\u001b[39m synchronous_parallel_sample(\n\u001b[0;32m    399\u001b[0m             worker_set\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworkers,\n\u001b[0;32m    400\u001b[0m             max_agent_steps\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mtrain_batch_size,\n\u001b[0;32m    401\u001b[0m         )\n\u001b[0;32m    402\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 403\u001b[0m         train_batch \u001b[39m=\u001b[39m synchronous_parallel_sample(\n\u001b[0;32m    404\u001b[0m             worker_set\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mworkers, max_env_steps\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mtrain_batch_size\n\u001b[0;32m    405\u001b[0m         )\n\u001b[0;32m    407\u001b[0m train_batch \u001b[39m=\u001b[39m train_batch\u001b[39m.\u001b[39mas_multi_agent()\n\u001b[0;32m    408\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_counters[NUM_AGENT_STEPS_SAMPLED] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m train_batch\u001b[39m.\u001b[39magent_steps()\n",
      "File \u001b[1;32mc:\\Users\\nadir\\anaconda3\\envs\\GTRay2.4_torch_tf\\lib\\site-packages\\ray\\rllib\\execution\\rollout_ops.py:85\u001b[0m, in \u001b[0;36msynchronous_parallel_sample\u001b[1;34m(worker_set, max_agent_steps, max_env_steps, concat)\u001b[0m\n\u001b[0;32m     82\u001b[0m     sample_batches \u001b[39m=\u001b[39m [worker_set\u001b[39m.\u001b[39mlocal_worker()\u001b[39m.\u001b[39msample()]\n\u001b[0;32m     83\u001b[0m \u001b[39m# Loop over remote workers' `sample()` method in parallel.\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 85\u001b[0m     sample_batches \u001b[39m=\u001b[39m worker_set\u001b[39m.\u001b[39;49mforeach_worker(\n\u001b[0;32m     86\u001b[0m         \u001b[39mlambda\u001b[39;49;00m w: w\u001b[39m.\u001b[39;49msample(), local_worker\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, healthy_only\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[0;32m     87\u001b[0m     )\n\u001b[0;32m     88\u001b[0m     \u001b[39mif\u001b[39;00m worker_set\u001b[39m.\u001b[39mnum_healthy_remote_workers() \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     89\u001b[0m         \u001b[39m# There is no point staying in this loop, since we will not be able to\u001b[39;00m\n\u001b[0;32m     90\u001b[0m         \u001b[39m# get any new samples if we don't have any healthy remote workers left.\u001b[39;00m\n\u001b[0;32m     91\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nadir\\anaconda3\\envs\\GTRay2.4_torch_tf\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py:713\u001b[0m, in \u001b[0;36mWorkerSet.foreach_worker\u001b[1;34m(self, func, local_worker, healthy_only, remote_worker_ids, timeout_seconds, return_obj_refs, mark_healthy)\u001b[0m\n\u001b[0;32m    710\u001b[0m \u001b[39mif\u001b[39;00m local_worker \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlocal_worker() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    711\u001b[0m     local_result \u001b[39m=\u001b[39m [func(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlocal_worker())]\n\u001b[1;32m--> 713\u001b[0m remote_results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__worker_manager\u001b[39m.\u001b[39;49mforeach_actor(\n\u001b[0;32m    714\u001b[0m     func,\n\u001b[0;32m    715\u001b[0m     healthy_only\u001b[39m=\u001b[39;49mhealthy_only,\n\u001b[0;32m    716\u001b[0m     remote_actor_ids\u001b[39m=\u001b[39;49mremote_worker_ids,\n\u001b[0;32m    717\u001b[0m     timeout_seconds\u001b[39m=\u001b[39;49mtimeout_seconds,\n\u001b[0;32m    718\u001b[0m     return_obj_refs\u001b[39m=\u001b[39;49mreturn_obj_refs,\n\u001b[0;32m    719\u001b[0m     mark_healthy\u001b[39m=\u001b[39;49mmark_healthy,\n\u001b[0;32m    720\u001b[0m )\n\u001b[0;32m    722\u001b[0m handle_remote_call_result_errors(remote_results, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ignore_worker_failures)\n\u001b[0;32m    724\u001b[0m \u001b[39m# With application errors handled, return good results.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nadir\\anaconda3\\envs\\GTRay2.4_torch_tf\\lib\\site-packages\\ray\\rllib\\utils\\actor_manager.py:599\u001b[0m, in \u001b[0;36mFaultTolerantActorManager.foreach_actor\u001b[1;34m(self, func, healthy_only, remote_actor_ids, timeout_seconds, return_obj_refs, mark_healthy)\u001b[0m\n\u001b[0;32m    590\u001b[0m     func, remote_actor_ids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_filter_func_and_remote_actor_id_by_state(\n\u001b[0;32m    591\u001b[0m         func, remote_actor_ids\n\u001b[0;32m    592\u001b[0m     )\n\u001b[0;32m    594\u001b[0m remote_calls \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__call_actors(\n\u001b[0;32m    595\u001b[0m     func\u001b[39m=\u001b[39mfunc,\n\u001b[0;32m    596\u001b[0m     remote_actor_ids\u001b[39m=\u001b[39mremote_actor_ids,\n\u001b[0;32m    597\u001b[0m )\n\u001b[1;32m--> 599\u001b[0m _, remote_results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__fetch_result(\n\u001b[0;32m    600\u001b[0m     remote_actor_ids\u001b[39m=\u001b[39;49mremote_actor_ids,\n\u001b[0;32m    601\u001b[0m     remote_calls\u001b[39m=\u001b[39;49mremote_calls,\n\u001b[0;32m    602\u001b[0m     tags\u001b[39m=\u001b[39;49m[\u001b[39mNone\u001b[39;49;00m] \u001b[39m*\u001b[39;49m \u001b[39mlen\u001b[39;49m(remote_calls),\n\u001b[0;32m    603\u001b[0m     timeout_seconds\u001b[39m=\u001b[39;49mtimeout_seconds,\n\u001b[0;32m    604\u001b[0m     return_obj_refs\u001b[39m=\u001b[39;49mreturn_obj_refs,\n\u001b[0;32m    605\u001b[0m     mark_healthy\u001b[39m=\u001b[39;49mmark_healthy,\n\u001b[0;32m    606\u001b[0m )\n\u001b[0;32m    608\u001b[0m \u001b[39mreturn\u001b[39;00m remote_results\n",
      "File \u001b[1;32mc:\\Users\\nadir\\anaconda3\\envs\\GTRay2.4_torch_tf\\lib\\site-packages\\ray\\rllib\\utils\\actor_manager.py:467\u001b[0m, in \u001b[0;36mFaultTolerantActorManager.__fetch_result\u001b[1;34m(self, remote_actor_ids, remote_calls, tags, timeout_seconds, return_obj_refs, mark_healthy)\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m remote_calls:\n\u001b[0;32m    465\u001b[0m     \u001b[39mreturn\u001b[39;00m [], RemoteCallResults()\n\u001b[1;32m--> 467\u001b[0m ready, _ \u001b[39m=\u001b[39m ray\u001b[39m.\u001b[39;49mwait(\n\u001b[0;32m    468\u001b[0m     remote_calls,\n\u001b[0;32m    469\u001b[0m     num_returns\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(remote_calls),\n\u001b[0;32m    470\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    471\u001b[0m     \u001b[39m# Make sure remote results are fetched locally in parallel.\u001b[39;49;00m\n\u001b[0;32m    472\u001b[0m     fetch_local\u001b[39m=\u001b[39;49m\u001b[39mnot\u001b[39;49;00m return_obj_refs,\n\u001b[0;32m    473\u001b[0m )\n\u001b[0;32m    475\u001b[0m \u001b[39m# Remote data should already be fetched to local object store at this point.\u001b[39;00m\n\u001b[0;32m    476\u001b[0m remote_results \u001b[39m=\u001b[39m RemoteCallResults()\n",
      "File \u001b[1;32mc:\\Users\\nadir\\anaconda3\\envs\\GTRay2.4_torch_tf\\lib\\site-packages\\ray\\_private\\client_mode_hook.py:105\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[39mif\u001b[39;00m func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39minit\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[0;32m    104\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(ray, func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 105\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nadir\\anaconda3\\envs\\GTRay2.4_torch_tf\\lib\\site-packages\\ray\\_private\\worker.py:2719\u001b[0m, in \u001b[0;36mwait\u001b[1;34m(object_refs, num_returns, timeout, fetch_local)\u001b[0m\n\u001b[0;32m   2717\u001b[0m timeout \u001b[39m=\u001b[39m timeout \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m10\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m6\u001b[39m\n\u001b[0;32m   2718\u001b[0m timeout_milliseconds \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(timeout \u001b[39m*\u001b[39m \u001b[39m1000\u001b[39m)\n\u001b[1;32m-> 2719\u001b[0m ready_ids, remaining_ids \u001b[39m=\u001b[39m worker\u001b[39m.\u001b[39;49mcore_worker\u001b[39m.\u001b[39;49mwait(\n\u001b[0;32m   2720\u001b[0m     object_refs,\n\u001b[0;32m   2721\u001b[0m     num_returns,\n\u001b[0;32m   2722\u001b[0m     timeout_milliseconds,\n\u001b[0;32m   2723\u001b[0m     worker\u001b[39m.\u001b[39;49mcurrent_task_id,\n\u001b[0;32m   2724\u001b[0m     fetch_local,\n\u001b[0;32m   2725\u001b[0m )\n\u001b[0;32m   2726\u001b[0m \u001b[39mreturn\u001b[39;00m ready_ids, remaining_ids\n",
      "File \u001b[1;32mpython\\ray\\_raylet.pyx:1870\u001b[0m, in \u001b[0;36mray._raylet.CoreWorker.wait\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpython\\ray\\_raylet.pyx:201\u001b[0m, in \u001b[0;36mray._raylet.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if not debugAsGym and not testResult:\n",
    "    N = 1000\n",
    "\n",
    "    for n in range(N):\n",
    "        result = algo.train()\n",
    "        print(\"Loop: \", n)\n",
    "        if n % 10 == 0:\n",
    "            print(\"Saved\", n)\n",
    "            algo.save()\n",
    "            \n",
    "    algo.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not debugAsGym and not testResult:\n",
    "    N = 1000\n",
    "\n",
    "    for n in range(N):\n",
    "        result = algo.train()\n",
    "        print(\"Loop: \", n)\n",
    "        if n % 50 == 0:\n",
    "            print(\"Saved\", n)\n",
    "            algo.save()\n",
    "            \n",
    "    algo.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from myRTClass import MyGranTurismoRTGYM, DEFAULT_CONFIG_DICT\n",
    "import gymnasium\n",
    "\n",
    "my_config = DEFAULT_CONFIG_DICT\n",
    "my_config[\"interface\"] = MyGranTurismoRTGYM\n",
    "my_config[\"time_step_duration\"] = 0.1\n",
    "my_config[\"start_obs_capture\"] = 0.1\n",
    "my_config[\"time_step_timeout_factor\"] = 1.0\n",
    "#my_config[\"ep_max_length\"] = 224\n",
    "my_config[\"act_buf_len\"] = 3\n",
    "my_config[\"reset_act_buf\"] = False\n",
    "my_config[\"benchmark\"] = True\n",
    "my_config[\"benchmark_polyak\"] = 0.2\n",
    "\n",
    "my_config[\"interface_kwargs\"] = {\n",
    "  'debugFlag': False, # do not use render() while True\n",
    "  'img_hist_len': 3,\n",
    "  'modelMode': 4,\n",
    "  'agent' : 'PPO',\n",
    "  #  [42, 42, K], [84, 84, K], [10, 10, K], [240, 320, K] and  [480, 640, K]\n",
    "  'imageWidth' : 42, # there is a default Cov layer for PPO with 240 x 320\n",
    "  'imageHeight' : 42,\n",
    "  'trackChoice' : 3, # 1 is High Speed Ring, 2 is 0-400m, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not debugAsGym and testResult:\n",
    "    def env_creator(env_config):\n",
    "        env = gymnasium.make(\"real-time-gym-v1\", config=env_config)\n",
    "        return env  # return an env instance\n",
    "\n",
    "    from ray.tune.registry import register_env\n",
    "    register_env(\"gt-rtgym-env-v1\", lambda config: env_creator(my_config)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not debugAsGym and testResult:\n",
    "    from ray.rllib.algorithms.algorithm import Algorithm\n",
    "    algo = Algorithm.from_checkpoint(\"C:/Users/nadir/ray_results/PPO_gt-rtgym-env-v1_2023-05-19_07-37-37z3d6v2w2/checkpoint_000061\")\n",
    "    #algo = Algorithm.from_checkpoint(\"C:/Users/nadir/ray_results/PPO_gt-rtgym-env-v1_2023-05-19_07-37-37z3d6v2w2/checkpoint_002000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not debugAsGym and testResult:\n",
    "    result = algo.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not debugAsGym and testResult:\n",
    "\n",
    "    policy = algo.get_policy()\n",
    "    #print(policy.model)\n",
    "    model = policy.model\n",
    "    print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GTAI2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
